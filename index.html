<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"melrose-lbt.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="Hexo">
<meta property="og:url" content="https://melrose-lbt.github.io/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:locale" content="en_US">
<meta property="article:author" content="John Doe">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="https://melrose-lbt.github.io/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'en'
  };
</script>

  <title>Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/20/Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/20/Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks/" class="post-title-link" itemprop="url">【Paper Reading Note】Reducing the dimensionality of data with neural networks</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-20 19:23:22 / Modified: 20:07:44" itemprop="dateCreated datePublished" datetime="2022-10-20T19:23:22+08:00">2022-10-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-reading-note/" itemprop="url" rel="index"><span itemprop="name">Paper reading note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="【Paper-Reading-Note】Reducing-the-dimensionality-of-data-with-neural-networks"><a href="#【Paper-Reading-Note】Reducing-the-dimensionality-of-data-with-neural-networks" class="headerlink" title="【Paper Reading Note】Reducing the dimensionality of data with neural networks"></a>【Paper Reading Note】Reducing the dimensionality of data with neural networks</h1><p>[TOC]</p>
<p><strong>Author</strong>: <strong>G.E. Hinton and R.R. Salakhutdinov</strong></p>
<blockquote>
<p><strong>Abstract</strong>: High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “auto encoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep auto encoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.</p>
</blockquote>
<h2 id="1-Background-Knowledge"><a href="#1-Background-Knowledge" class="headerlink" title="_1. Background Knowledge_"></a><strong>_1. Background Knowledge_</strong></h2><h3 id="1-1-Boltzman-machine-amp-Restricted-Boltzman-machine"><a href="#1-1-Boltzman-machine-amp-Restricted-Boltzman-machine" class="headerlink" title="_1.1 Boltzman machine &amp; Restricted Boltzman machine_"></a><strong>_1.1 Boltzman machine &amp; Restricted Boltzman machine_</strong></h3><p>The first problem is what is the Boltzman machine. A <strong>Boltzmann machine</strong> is a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off. A <strong>Restricted Boltzman machine</strong> is a network like the two layers of fully connected layer. The visualization of two machines are shown in <strong>Fig. 1</strong>.</p>
<p><img src="/image/article/RBM.png" alt=""></p>
<center>Fig.1 Visualization of BM and RBM.</center>

<p>In this article, we mainly focus on the <strong>Restricted Boltzman machine</strong> (<strong>RBM</strong> in short).</p>
<h4 id="Mechanism-of-RBM"><a href="#Mechanism-of-RBM" class="headerlink" title="_Mechanism of RBM_"></a><strong>_Mechanism of RBM_</strong></h4><p>The <strong>Restricted Boltzman machine</strong> has two layer. The first layer is the visible layer, and the second layer is the hidden layer. Each layer has several nodes. Each node has two states, 1 or 0. We simply called these layer $v_i$, $h_j$, which means the ith node for the visible layer and the jth node for the hidden layer. The weight between the node $v_i$ and $h_j$ is $w_{ij}$. Each node also has bias.</p>
<p>The total energy for the given state vectors $\vec{v},\vec{h}$ is :</p>
<script type="math/tex; mode=display">
E(\vec{v}, \vec{h})= -\sum_ia_iv_i-\sum_jb_jh_j-\sum_{i,j}v_ih_jw_{ij}</script><p><strong>RBM</strong> is a probability-based model. In each time step, <strong>RBM</strong> has a certain state which can be descriped as $(\vec{v}, \vec{h})$. The probability distribution is :</p>
<script type="math/tex; mode=display">
p(\vec{v}, \vec{h})=\frac{e^{-E(\vec{v}, \vec{h} ) } }{\sum_{\vec{v}, \vec{h}}e^{-E(\vec{v}, \vec{h})}}</script><p>This is so called <strong>Boltzman distribution</strong>. It gives the probability when the particle in a certain state.</p>
<p>However, to calculate the (2) distribution is difficult. But to calculate the conditional distribution is relatively easier.</p>
<script type="math/tex; mode=display">
p(\vec{h}|\vec{v})=\prod_ip(h_i|\vec{v}) \\
p(\vec{v}|\vec{h})=\prod_ip(v_i|\vec{h})</script><p>Whatever the BM or the RBM, the state of a unit or you could say neuron could be 0 or 1. The probability to be 1 for each unit is:</p>
<script type="math/tex; mode=display">
p(h_j=1|\vec{v}) = \sigma(b_i + \sum_i v_iw_{ij})</script><p>Where $w_{ij}$ is the weight on the connection between unit $i$ nad $j$, and $v_i$ is the state of the unit, 1is on and 0 is off. $\sigma()$ function is:</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1+e^{-x}}</script><p>The detail information of how to get (4) please refer to <a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6530523.html">Here</a>.</p>
<h4 id="Training-Method-Gibbs-Sampling-amp-Contrastive-Divergence"><a href="#Training-Method-Gibbs-Sampling-amp-Contrastive-Divergence" class="headerlink" title="_Training Method: Gibbs Sampling &amp; Contrastive Divergence_"></a><strong>_Training Method: Gibbs Sampling &amp; Contrastive Divergence_</strong></h4><p>For the Constrastive Divergence algorithm please refer to <a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf">Notes On CD</a> and <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36607894/article/details/96635664">This</a>. </p>
<p>So for the gradient of $w_{ij}$, the equation is:</p>
<script type="math/tex; mode=display">
\Delta{w_{ij}} = \epsilon \times(<v_ih_j>_{data}-<v^{'}_ih^{'}_j>_{reconstruction})</script><h3 id="1-2-Maximum-Likelihood-Approximation"><a href="#1-2-Maximum-Likelihood-Approximation" class="headerlink" title="_1.2 Maximum Likelihood Approximation_"></a><strong>_1.2 Maximum Likelihood Approximation_</strong></h3><p><strong>MLA</strong> can approximate the model’s parameters when you have defined which model you chose. </p>
<p><img src="/image/article/MaxLike.jpeg" alt=""></p>
<center>
  Fig. 2 The visualization of maximum likelihood method.
</center>

<p>We can see <strong>RBM</strong> as a black box. Given an input vector and finally it will output a reconstructed vector. How do we guess the model parameters based on the information that we already know? Well, as we know, we have already understand the fundemental mechanism of the <strong>RBM</strong>. So we know its model, but we don’t know its parameters. Here, the maximum likelihood method can be used as shown in Fig. 2.</p>
<p>In the <strong>RBM</strong>, we want the output equals to the input. So, the input and label in this case are (<strong>X</strong>, <strong>X</strong>) because <strong>Y</strong>=<strong>X</strong>. Hence, we could derive the equation of $P(\vec{v_{out}})$:</p>
<script type="math/tex; mode=display">
P(\vec{v_{out}})= \sum_hP(\vec{v}, \vec{h})=\frac{\sum_he^{-E(\vec{v_{out},\vec{h}})}}{\sum_{\vec{v},\vec{h}}e^{-E(\vec{v}, \vec{h})}}</script><p>Write it in a log form:</p>
<script type="math/tex; mode=display">
L(W,a,b)=-\sum_iln(P(\vec{v^{(i)}_{out}}))</script><h2 id="2-Content"><a href="#2-Content" class="headerlink" title="_2. Content_"></a><strong>_2. Content_</strong></h2><p>The autoencoder presented by Hinton is used for dimensionality reduction. Compared with PCA, which finds the directions of greatest variance in the dataset and represents each data point by its coordinates along each of these directions. The visualization of an encoder is shown in Fig. 3.</p>
<p><img src="/image/article/encoder.png" alt=""></p>
<h3 id="2-1-The-current-issue-of-optimizing-the-autoencoder"><a href="#2-1-The-current-issue-of-optimizing-the-autoencoder" class="headerlink" title="_2.1 The current issue of optimizing the autoencoder_"></a><strong>_2.1 The current issue of optimizing the autoencoder_</strong></h3><p>When a autoencode has multiple layers, it is difficult to optimize the weight.</p>
<blockquote>
<p>If the initial weights are large, autoencoders will find poor local minima;</p>
<p>If the initial weights are small, the gradients will vanish.</p>
</blockquote>
<p>If the initial weights are close to a good solution, gradient descent works very well.</p>
<p><strong>QUESTION</strong>: How to find this initial weights?</p>
<p><strong>SOLUTION</strong>: Use pretraining procedure one layer at a time.</p>
<h3 id="2-2-The-pretrain-method"><a href="#2-2-The-pretrain-method" class="headerlink" title="_2.2 The pretrain method_"></a><strong>_2.2 The pretrain method_</strong></h3><p>Use <strong>RBM</strong> as an unit to train, and the contrastive divergence as learning method.</p>
<h3 id="2-3-Layer-by-layer-architecture"><a href="#2-3-Layer-by-layer-architecture" class="headerlink" title="_2.3 Layer by layer architecture_"></a><strong>_2.3 Layer by layer architecture_</strong></h3><p>After trained a <strong>RBM</strong>, we can use the hidden layer of the first <strong>RBM</strong> as the visible layer for the next <strong>RBM</strong>. You can connect it as many as possible. </p>
<h3 id="2-4-So-called-Deep-Belief-Network"><a href="#2-4-So-called-Deep-Belief-Network" class="headerlink" title="_2.4 So called Deep Belief Network_"></a><strong>_2.4 So called Deep Belief Network_</strong></h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37520918/article/details/102738465">Deep Belief Network</a></p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/Intro/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/Intro/" class="post-title-link" itemprop="url">Intro</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-15 19:23:22 / Modified: 22:00:52" itemprop="dateCreated datePublished" datetime="2022-10-15T19:23:22+08:00">2022-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Diary/" itemprop="url" rel="index"><span itemprop="name">Diary</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Introduce-Myself"><a href="#Introduce-Myself" class="headerlink" title="Introduce Myself"></a>Introduce Myself</h1><blockquote>
<p>HI，我是Melrose。是一名USST大四在校封控生，本科专业是智能科学与技术，目前已保研至UESTC，未来三年会继续向计算机和智能方向发展。热衷于学习CS和AI理论和系统相关的知识，对AI能否产生意识有强烈的兴趣。同时我喜欢英语（无论听说读写），我认为英语不是一个学科，而是去学习和了解更广阔的知识和世界钥匙。</p>
</blockquote>
<p>首先，欢迎来到我的博客！技术博客肯定是Coder的标配啦。在这里我会持续记录和分享我的学习生活以及一些相关的项目和技术。如果想了解我所完成的一些简单的项目，欢迎进入我的Github仓库（页面右下角）。</p>
<p>在这篇日记想记录一下大学三年来的心路历程和对学习这件事的看法。</p>
<h2 id="大学三年"><a href="#大学三年" class="headerlink" title="大学三年"></a>大学三年</h2><h4 id="大一"><a href="#大一" class="headerlink" title="大一"></a>大一</h4><p>2019年6月，在经历三年漫长的高中学习后，一切都落下了帷幕。6月25日凌晨5点得知自己不理想的成绩之后开始了一段时间的消沉。7月份在旅游回家的绿皮火车上，搜索着如何在即将前往的上海实现自我救赎。这时候看到了插班生考试，堪称人生第二次高考机会。激动不已的我仿佛抓住了救命稻草，被高考失败冲昏了头脑的我马上立下壮志，开始了长达一年的数学英语的学习。2020年7月8日，高考的第二天，我走出了插班生考场，搭上了回校的公交车。失败是肯定的。大一一年，所有的精力都在考研竞赛难度的高数以及考研托福难度的英语上。</p>
<p>若要硬说我这一年收获了什么，莫过于通过又一次的失败明白了个人能力的欠缺还有那残留脑海中的鸡汤和若干英语词汇。也很感激我大一所做的一切，大二开始，我利用大一积累的阅读能力开始阅读外刊，比如经济学人、时代周刊等。每每想精读一篇的时候，就会打印出来和读语文课文一样分析每句话每段的意思。大一备考英语的寒假，每晚8点准时打开手机放Fox News的广播持续三小时。值得高兴的是，三个月后我逐渐理解了广播中的人在说些什么。如今大四，关于英语的听说读写仍然坚持着，听纯英课程和阅读书籍基本无压力。</p>
<h4 id="大二"><a href="#大二" class="headerlink" title="大二"></a>大二</h4><p>大二一整年的时间，把经历放在了电子设计大赛上。电子设计大赛是单数年国赛，双数年省赛赛制。刚开始接触嵌入式的小白，仅仅有大一学到的一点C语言基础，在学习STM32时可谓是处处碰壁。RAM？内核？系统总线？寄存器？每天在实验室面向百度学习，把各种各样的知识查了个遍。从万物起源的智能小车做起，电机驱动？主控芯片？PWM？从20年12月开始的小车项目，由于消极怠工，直至21年4月份才初步完工。随后又完成了板球控制系统、基于OpenMV的智能无人车、风力摆控制系统等等。2021年11月，经过四天三夜通宵鏖战，最后取得上海赛区二等奖。</p>
<p>结果虽然不尽人意，但学习过程简直是相当Amazing啊！大一结束我还是只会做高数和英语题、会敲Hello world的小白，但大二结束时，由于备赛过程的知识学习，让我意识到学校所学的数学、模拟电路、数字电路、微控制器、计算机组成、操作系统、数据结构、自动控制原理和算法，简直是浑然一体的学科！运用这些知识搭建的复杂系统和解决问题之后具有强烈的满足感和充实感。同时，在备赛基础上，还扩展了解了FreeRTOS实时操作系统、PCB电路板设计和绘制、LVGL嵌入式UI设计。期间在b站遇到了稚辉君，全栈工程师天花板。工程能力和开发能力是我的目标。今后去了电子科大读研也算是校友了哈哈哈。</p>
<h4 id="大三"><a href="#大三" class="headerlink" title="大三"></a>大三</h4><p>大二考虑保研，大三整整一年都在为能争取到校内名额准备。学校课程和竞赛科研都在齐头并进。给大三一个主题的话，是“科研+开发”莫属了。加入了科研课题组，每周做科研汇报和组会，有幸做了一次全英的论文汇报。22年3月开始图像散斑重建的课题，代码构建、调试、结果统计、论文撰写、投递、改稿，这一系列的过程持续到今年10月底。在保研竞争最紧张的阶段，感谢老师和师兄的帮助让我完成了学术生涯第一篇论文，也是第一篇SCI。一年的类研究生生活，让我明白自己在专业技能、论文写作、知识总结、运用和创新方面仍然有很大差距，同时也被科研团队协作和讨论的氛围吸引，这也是我选择继续读研的重要原因。保研不焦虑是不可能的，专业一般只保2-3人，我大三下学期开学的排名还是第四名。每日每夜都在担心自己能否成功保研。8月份9月份那段时间等结果简直是度日如年。好在顺利获取了校内名额，拿到了中南大学、电子科技大学、南京理工大学、华东理工大学和南京航空航天大学的offer。</p>
<h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>回顾这三年，自己的努力也算没有白费。每一年的几乎每一天，都过的满满当当、忙忙碌碌。大一学习之余当了班长管理班务，大二竞赛之余在就业指导中心做学生助理，大三科研之余当了一个多月的大白志愿者。尽管如此，还是有很多遗憾：尝试三年申请进博会志愿者均失败（大一没有名额，大二已经入选但学院又没有让大二参加，大三又一次入选结果撞上了电子设计竞赛，大四实在是需要实习没有办法）；没有好好把握很多竞赛，都水了过去；没有好好静下来看看这座城市和校园… 青春最有活力的三年献给了学习去弥补三年前的那次失败，但我并不后悔。</p>
<h2 id="我们为何学习"><a href="#我们为何学习" class="headerlink" title="我们为何学习"></a>我们为何学习</h2><p>“只有学习，才能过上美好的生活。”</p>
<p>“什么年龄该干什么事。”</p>
<p>“学习就是为了工作，工作就是为了赚钱。”</p>
<p>“赚钱才能有更好的生活。”</p>
<p>从小到大，环境交给我的是这样的理念。好在我出生在互联网时代，有幸高中时期在b站遇到了西安电子科技大学教师——YJango。他的视频改变了我对学习的看法，能让我在青春的时候对科研和技术无比憧憬，感叹大自然的伟大和人类智慧的结，也给了我一定的勇气去抗争环境对学习的一些偏见。</p>
<p>我始终认为，学习目的不是为了更好的生活和赚钱。金钱和美好生活是学习的附属产物，工作是赚钱的手段，金钱是物质生活的基础。学习，能够让我们获取前人的知识和对世界的观念、能让我们能拥有批判性思维去看待事物、能让我们意识到个体的渺小、能让我们认识到生活不止米面柴油，还有星辰大海。</p>
<p>大自然如何运转、相互之间的关系是什么；互联网有多庞大，又为何如此迅速和稳定；宇宙运行的规律是什么；我们是唯一的智能体么；人工智能究竟会不会产生意识，我们还有多远…</p>
<p>互联网让我认识到，世界上千千万万的人一生都在重复同样的事情：上学、工作、赚钱、买车买房、退休。我们纵然无法全然地跳脱出这样的框架，毕竟个体脱离了社会什么也不是，但好在学习和探索，可以让你我在思想维度上感受与别人不一样的人生和世界。</p>
<p>中考高考考插的失败让我认识到，我也是千千万万人中的一个。</p>
<p>我想，人的生命只有一次，而且大多数都活在别人制定好的框架中活的不明不白。那么为何，不努力学习，有机会去世界不同的地方看一看，让自己有限的生命，更与众不同一点呢？</p>
<p>当然，这是作为一个理想主义者的我，在无忧无虑的大四时期的幼稚又不现实的观点。</p>
<p>但我仍然希望，未来不论如何，坚持终身学习的观念，并尽量客观的看待周围的一切事物。</p>
<p>这也是我Github上的座右铭，尝试达到相对客观的由来。</p>
<p>写于21岁，2022年10月15日晚，被封在宿舍。</p>
<blockquote>
<p>贴几个我喜欢的句子</p>
<p>“人类的遗产是知识的更新。”</p>
<p>“从有限的例子中学习模型以应对无限情况。”</p>
</blockquote>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/%E3%80%90%E5%8A%A8%E6%89%8B%E5%BC%80%E5%8F%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%97%A5%E8%AE%B0%E3%80%91GPU%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86API/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/%E3%80%90%E5%8A%A8%E6%89%8B%E5%BC%80%E5%8F%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%97%A5%E8%AE%B0%E3%80%91GPU%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86API/" class="post-title-link" itemprop="url">【动手开发深度学习框架日记】GPU内存管理API</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>

              <time title="Created: 2022-10-15 19:17:25" itemprop="dateCreated datePublished" datetime="2022-10-15T19:17:25+08:00">2022-10-15</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">Edited on</span>
                <time title="Modified: 2022-10-20 17:30:08" itemprop="dateModified" datetime="2022-10-20T17:30:08+08:00">2022-10-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Neutron/" itemprop="url" rel="index"><span itemprop="name">Neutron</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>在【Tensor基本数据结构】一文中，Tensor类会将数据分为在CPU端计算还是在GPU端计算。对应的就是numpy或Quark子数据结构。Tensor实现了GPU内存数据到CPU数据、CPU到GPU的迁移，通过调用<code>cpu()</code>和<code>gpu()</code>完成。数据的 Host to Device 以及 Device to Host 行为由CUDA提供，因此可以通过封装CUDA API供Python调用。本文主要解释CUDA的底层实现以及Python端的调用。</p>
</blockquote>
<p>@<a href="Content">TOC</a></p>
<h1 id="一、CUDA加速计算编程模型"><a href="#一、CUDA加速计算编程模型" class="headerlink" title="一、CUDA加速计算编程模型"></a>一、CUDA加速计算编程模型</h1><p>若要使用CUDA来对矩阵运算进行GPU加速，就需要遵循一定的编程模型（建议先阅读 CUDA By Example，很好的入门CUDA编程书籍）。</p>
<p>首先我们创建了一个数据结构，数据结构中包含数据、形状、维度等等信息。CUDA计算使用的是GPU（Host）端的数据，那么我们怎么把创建好在CPU的数据放到GPU中呢？CUDA提供了一个简便的API，<code>cudaMemcpy()</code>。该API允许数据从Host复制到Device，同时也允许Device的数据复制到Host，需要传入数据的指针，数据大小还有你的选择（从cpu到gpu还是gpu到cpu）。有了这个工具呢，我们还需要一个可以在GPU内存上申请空间的函数，也就是<code>cudaMalloc()</code>，该函数需要传入数据指针和数据的大小。那么，大小如何得到？这时候我们就需要根据数据结构的shape、dim等参数进行计算。最后一个问题，如何释放GPU内存的空间？使用<code>cudaFree()</code>可以解决这个问题。</p>
<p>总结来说，想要利用CUDA实现GPU加速计算，要有一下几个步骤：</p>
<blockquote>
<p><strong>1. 创建数据，填补相关的数据信息（CPU端）</strong><br><strong>2. 创建GPU数据内存指针</strong><br><strong>3. <code>cudaMalloc()</code>分配GPU内存，需要计算数据的size</strong><br><strong>4. <code>cudaMemcpy()</code>将CPU的数据复制到GPU对应内存</strong><br><strong>5. 实现CUDA kernel函数，传入必要参数计算</strong><br><strong>6. <code>cudaMemcpy()</code>将计算结果从GPU复制回CPU</strong><br><strong>7. 完成计算后释放内存</strong></p>
</blockquote>
<h1 id="二、内存管理API封装"><a href="#二、内存管理API封装" class="headerlink" title="二、内存管理API封装"></a>二、内存管理API封装</h1><p>上述的步骤其实是可以直接写在C++后端的算子API里的，但是为了Tensor管理的模块化和灵活性，打算将这些功能分散开，在创建GPU数据时完成对应的操作，之后再调用算子就是直接调用GPU内存进行计算了。</p>
<p>首先创建一个专门定义内存操作的.cu文件，我把它叫 MemSchedulor.cu。</p>
<p>为了方便筛查错误，我们需要定义一个宏，每当使用CUDA API时，在外面裹一层就好了。</p>
<pre><code class="lang-cpp">#define CUDA_CHECK(func)                                                        \
  &#123;                                                                            \
    cudaError_t e = (func);                                                    \
    assert((e == cudaSuccess) || (e == cudaErrorCudartUnloading));             \
  &#125;
</code></pre>
<p>然后我们先封装内存分配函数，这里实现了CPU的封装和GPU的封装。</p>
<pre><code class="lang-cpp">extern &quot;C&quot; float *AllocateDeviceData(int size)&#123;
    float *dev_data;
    CUDA_CHECK(cudaMalloc((void **)&amp;dev_data, size));
    return dev_data;
&#125;

extern &quot;C&quot; float *AllocateHostData(int size)&#123;
    float *host_data = (float *)malloc(size);
    return host_data;
&#125;
</code></pre>
<p>函数内部实现都非常简单，就是创建一个指针变量，分配数据然后返回，在Python端接收即可。需要注意这里输入参数size代表的是字节数，那我们就需要实现一个自动根据shape和dim计算size的函数。</p>
<pre><code class="lang-cpp">extern &quot;C&quot; inline int getSize(int dim, int *shape)&#123;
    // float32 by default, 4 bytes
    int size = 1;
    for(int i=0; i &lt; dim; i++)&#123;
        size = size * shape[i];
        &#125;
    return size * 4;
&#125;
</code></pre>
<p>整个框架的数据类型是基于float32的，所以这里要乘4作为数据所占内存的最终大小。</p>
<p>释放内存的封装也很简单。</p>
<pre><code class="lang-cpp">extern &quot;C&quot; void FreeDeviceData(float *data)&#123;
    CUDA_CHECK(cudaFree(data));
&#125;

extern &quot;C&quot; void FreeHostData(float *data)&#123;
    free(data);
&#125;
</code></pre>
<p>最后就是要实现数据的复制功能了，这里统一实现一个API，根据输入的参数来判定移动的方向。</p>
<pre><code class="lang-cpp">extern &quot;C&quot; void CopyDataFromTo(float *from_data, float *to_data, Device from, Device to, int size)&#123;
    if(from == CPU &amp;&amp; to == GPU)&#123;
        CUDA_CHECK(cudaMemcpy(to_data, from_data, size, cudaMemcpyHostToDevice));
    &#125;
    else if(from == GPU &amp;&amp; to == CPU)&#123;
        float *dev_data = (float *)from_data;
        float *host_data = (float *)to_data;

        CUDA_CHECK(cudaMemcpy(host_data, dev_data, size, cudaMemcpyDeviceToHost));
    &#125;
&#125;
</code></pre>
<p>以上就是MemSchedulor.cu所有的API封装，通过编译命令编译成动态链接库，使用ctypes导入库后就可以调用了。目前实现数据的分配、复制以及计算是没有问题的，但不代表这是最终版本，还需要以后开发完全后确定。</p>
<p>这就是内存管理的全部内容啦，之后还会进一步更新后端算子的封装以及使用cuDNN实现卷积的前向计算和求梯度运算（因为实现起来有些问题就直接调用cuDNN库了哈哈）。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91%E8%AE%BE%E5%A4%87%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">【Thunder源码阅读】Python数据结构封装</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-15 19:09:22 / Modified: 19:20:28" itemprop="dateCreated datePublished" datetime="2022-10-15T19:09:22+08:00">2022-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Source-Code-Reading-Note/" itemprop="url" rel="index"><span itemprop="name">Source Code Reading Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="设备管理"><a href="#设备管理" class="headerlink" title="设备管理"></a>设备管理</h1><blockquote>
<p>在此之前了解一下C++的派生和类的关系，虚函数的定义，纯虚函数的定义，<a target="_blank" rel="noopener" href="https://blog.csdn.net/lj89168/article/details/6623096?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165884267316782184639343%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165884267316782184639343&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-1-6623096-null-null.142^v34^control,185^v2^control&amp;utm_term=virtual%20%3D0&amp;spm=1018.2226.3001.4187">纯虚函数</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/mayue_web/article/details/88406527?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165884293516782395355104%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165884293516782395355104&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~sobaiduend~default-2-88406527-null-null.142^v35^control,185^v2^control&amp;utm_term=C%2B%2B%20final&amp;spm=1018.2226.3001.4187">final用法</a>。</p>
</blockquote>
<h2 id="Device-api-h"><a href="#Device-api-h" class="headerlink" title="Device_api.h"></a>Device_api.h</h2><h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><pre><code class="lang-Cpp">namespace dlsys&#123;
    namespace runtime&#123;
        class DeviceAPI&#123;
            public:
                virtual ~DeviceAPI()&#123;&#125;
                    virtual void *AllocDataSpace(DLContext ctx, size_t size,
                                                    size_t alignment)=0;
                    virtual void FreeDataSpace(DLContext ctx, void *ptr)=0;
                    virtual void CopyDataFromTo(const void *from, void *to,
                                                size_t size, DLContext ctx_from,
                                                DLContext ctx_to, 
                                                DLStreamHandle stream)=0;
                    virtual void StreamSync(DLContext ctx, DLStreamHandle stream)
                                                =0;
        &#125;
    &#125;
&#125;
</code></pre>
<h4 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h4><p>Device_api的定义目的是为CPU设备和GPU设备提供一个基类，基类中采用纯虚函数的定义方式。定义了分配内存空间、释放内存空间、CPU-GPU数据的转移以及流处理（这部分先不去涉及）。</p>
<h2 id="cuda-device-api-cc-amp-cuda-device-api-h"><a href="#cuda-device-api-cc-amp-cuda-device-api-h" class="headerlink" title="cuda_device_api.cc &amp; cuda_device_api.h"></a>cuda_device_api.cc &amp; cuda_device_api.h</h2><ul>
<li><h4 id="h文件源码"><a href="#h文件源码" class="headerlink" title="h文件源码"></a>h文件源码</h4></li>
</ul>
<pre><code class="lang-Cpp">namespace dlsys&#123;
    namespace runtime&#123;
        class CUDADeviceAPI : public DeviceAPI&#123;
            public:
                void *AllocDataSpace(DLContext ctx, size_t size,
                                                size_t alignment) final;
                void FreeDataSpace(DLContext ctx, void *ptr)final;
                void CopyDataFromTo(const void *from, void *to,
                                            size_t size, DLContext ctx_from,
                                            DLContext ctx_to, 
                                            DLStreamHandle stream)final;
                void StreamSync(DLContext ctx, DLStreamHandle stream)
                                            final;
        &#125;
    &#125;
&#125;
</code></pre>
<ul>
<li><h4 id="h文件源码解析"><a href="#h文件源码解析" class="headerlink" title="h文件源码解析"></a>h文件源码解析</h4><p>CUDADeviceAPI作为DeviceAPI的子类，需要重写父类定义的纯虚函数。</p>
</li>
<li><h4 id="Cpp文件源码以及解析"><a href="#Cpp文件源码以及解析" class="headerlink" title="Cpp文件源码以及解析"></a>Cpp文件源码以及解析</h4><pre><code class="lang-Cpp">#define CUDA_CALL(func)
&#123;
  cudaError_t err = (func);
  assert((err == cudaSucces) || (e == cudaErrorCudartUnloading));
&#125;
</code></pre>
<p>定义了宏函数，用来判定每一个CUDA执行是否报错，若有报错则抛出异常。</p>
</li>
</ul>
<p>接下来这个文件在dlsys::runtime下，实体定义了CUDADeviceAPI类的函数。</p>
<pre><code class="lang-Cpp">static void GPUCopy(const void *from, void *to, size_t size,

                                        cudaMemcpyKind kind, cudaStream_t stream) &#123;
    if (stream != 0) &#123;    
        CUDA_CALL(cudaMemcpyAsync(to, from, size, kind, stream));
    &#125; 
    else &#123;
        CUDA_CALL(cudaMemcpy(to, from, size, kind));
    &#125;
&#125;
</code></pre>
<p>首先定义了GPU复制的函数，为CopyDataFromTo成员函数提供GPU的接口。</p>
<pre><code class="lang-Cpp">void *CUDADeviceAPI::AllocDataSpace(DLContext ctx, size_t size,

size_t alignment) &#123;

//std::cout &lt;&lt; &quot;allocating cuda data&quot; &lt;&lt; std::endl;

    CUDA_CALL(cudaSetDevice(ctx.device_id));

    assert((256 % alignment) == 0U); // &lt;&lt; &quot;CUDA space is aligned at 256 bytes&quot;;

    void *ret;

    CUDA_CALL(cudaMalloc(&amp;ret, size));

    return ret;

&#125;
</code></pre>
<p>AllocDataSpace首先根据DLContext的device_id设定指定的GPU（cudaSetDevice）。alignment负责将CUDA内存空间进行对齐。再定义一个任意类型的指针变量，ret，调用cudaMalloc，在GPU分配size大小的内存。</p>
<pre><code class="lang-Cpp">void CUDADeviceAPI::FreeDataSpace(DLContext ctx, void *ptr) &#123;

//std::cout &lt;&lt; &quot;releasing cuda data&quot; &lt;&lt; std::endl;

    CUDA_CALL(cudaSetDevice(ctx.device_id));

    CUDA_CALL(cudaFree(ptr));

&#125;
</code></pre>
<p>FreeDataSpace根据DLContext的device_id找到指定的GPU，调用cudaFree释放内存。</p>
<pre><code class="lang-Cpp">void CUDADeviceAPI::CopyDataFromTo(const void *from, void *to, size_t size,

DLContext ctx_from, DLContext ctx_to, DLStreamHandle stream) &#123;

//std::cout &lt;&lt; &quot;copying cuda data&quot; &lt;&lt; std::endl;

    cudaStream_t cu_stream = static_cast&lt;cudaStream_t&gt;(stream);

    if (ctx_from.device_type == kGPU &amp;&amp; ctx_to.device_type == kGPU) &#123;

        CUDA_CALL(cudaSetDevice(ctx_from.device_id));

    if (ctx_from.device_id == ctx_to.device_id) &#123;

        GPUCopy(from, to, size, cudaMemcpyDeviceToDevice, cu_stream);

    &#125; else &#123;

        cudaMemcpyPeerAsync(to, ctx_to.device_id, from, ctx_from.device_id,

        size, cu_stream);

    &#125;

    &#125; else if (ctx_from.device_type == kGPU &amp;&amp; ctx_to.device_type == kCPU) &#123;

        CUDA_CALL(cudaSetDevice(ctx_from.device_id));

        GPUCopy(from, to, size, cudaMemcpyDeviceToHost, cu_stream);

    &#125; else if (ctx_from.device_type == kCPU &amp;&amp; ctx_to.device_type == kGPU) &#123;

        CUDA_CALL(cudaSetDevice(ctx_to.device_id));

        GPUCopy(from, to, size, cudaMemcpyHostToDevice, cu_stream);

    &#125; else &#123;

        std::cerr &lt;&lt; &quot;expect copy from/to GPU or between GPU&quot; &lt;&lt; std::endl;

    &#125;

&#125;
</code></pre>
<p>CopyDataFromTo先进行逻辑判断，有四种情况：</p>
<ol>
<li>数据的来源和目的地都是GPU<br> 设置GPU设备（cudaSetDevice）。如果是在同一个GPU移动，直接调用GPUCopy。否则调用cudaMemcpyPeerAsync _（待完善和学习）_。</li>
<li>数据来自GPU，目的地是CPU<br> 直接调用GPUcopy。</li>
<li>数据来自CPU，目的地是GPU<br> 直接调用GPUcopy。</li>
<li>不允许执行CPU到CPU的，因为这是GPU的设备管理代码<br> 打印错误。</li>
</ol>
<pre><code class="lang-Cpp">void CUDADeviceAPI::StreamSync(DLContext ctx, DLStreamHandle stream) &#123;

    CUDA_CALL(cudaSetDevice(ctx.device_id));

    CUDA_CALL(cudaStreamSynchronize(static_cast&lt;cudaStream_t&gt;(stream)));

&#125;
</code></pre>
<p>_（待完善和学习Stream相关的知识）_。</p>
<h2 id="cpu-device-api-cc-amp-cpu-device-api-h"><a href="#cpu-device-api-cc-amp-cpu-device-api-h" class="headerlink" title="cpu_device_api.cc &amp; cpu_device_api.h"></a>cpu_device_api.cc &amp; cpu_device_api.h</h2><ul>
<li><h4 id="h文件源码-1"><a href="#h文件源码-1" class="headerlink" title="h文件源码"></a>h文件源码</h4><pre><code class="lang-Cpp">namespace dlsys&#123;

  namespace runtime&#123;

      class CPUDeviceAPI:public DeviceAPI&#123;
          public:
          void *AllocDataSpace(DLContext ctx, size_t size, size_t alignment) final;
          void FreeDataSpace(DLContext ctx, void *ptr) final;
          void CopyDataFromTo(const void *from, void *to, size_t size,
          DLContext ctx_from, DLContext ctx_to, DLStreamHandle stream) final;
          void StreamSync(DLContext ctx, DLStreamHandle stream) final;
      &#125;;
  &#125;
&#125;
</code></pre>
</li>
<li><h4 id="h文件源码解析-1"><a href="#h文件源码解析-1" class="headerlink" title="h文件源码解析"></a>h文件源码解析</h4></li>
</ul>
<hr>
<p>是对DeviceAPI的一次实体化。</p>
<ul>
<li><h4 id="Cpp文件源码以及解析-1"><a href="#Cpp文件源码以及解析-1" class="headerlink" title="Cpp文件源码以及解析"></a>Cpp文件源码以及解析</h4><p>CPU的设备管理的代码十分简单，如下</p>
<pre><code class="lang-Cpp">namespace dlsys&#123;

  namespace runtime&#123;

      void *CPUDeviceAPI::AllocDataSpace(DLContext ctx, size_t size,

      size_t alignment)&#123;
          void *ptr;
          int ret = posix_memalign(&amp;ptr, alignment, size);
          if(ret != 0)
          throw std::bad_alloc();
          return ptr;
      &#125;

      void CPUDeviceAPI::FreeDataSpace(DLContext ctx, void *ptr) &#123; free(ptr); &#125;

      void CPUDeviceAPI::CopyDataFromTo(const void *from, void *to, size_t size,
      DLContext ctx_from, DLContext ctx_to,
      DLStreamHandle stream) &#123;
          memcpy(to, from, size);
      &#125;

      void CPUDeviceAPI::StreamSync(DLContext ctx, DLStreamHandle stream) &#123;&#125;

  &#125;
&#125;
</code></pre>
</li>
</ul>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91GPU%E8%BF%90%E7%AE%97%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91GPU%E8%BF%90%E7%AE%97%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" class="post-title-link" itemprop="url">【Thunder源码阅读】Python数据结构封装</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-15 19:07:22 / Modified: 19:20:14" itemprop="dateCreated datePublished" datetime="2022-10-15T19:07:22+08:00">2022-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Source-Code-Reading-Note/" itemprop="url" rel="index"><span itemprop="name">Source Code Reading Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="GPU运算数据结构"><a href="#GPU运算数据结构" class="headerlink" title="GPU运算数据结构"></a>GPU运算数据结构</h1><blockquote>
<p>这里首先可以了解一下 <strong>cplusplus 和 extern “C”的含义</strong>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_36817189/article/details/110423243?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165884116316781790729085%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165884116316781790729085&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_positive~default-1-110423243-null-null.142^v34^control,185^v2^control&amp;utm_term=extern%20C&amp;spm=1018.2226.3001.4187">extern C</a>。</p>
</blockquote>
<h4 id="源码"><a href="#源码" class="headerlink" title="源码"></a>源码</h4><pre><code class="lang-Cpp">#ifdef __cplusplus
#define DLSYS_EXTERN_C extern &quot;C&quot;
#else
#define DLSYS_EXTERN_C
#endif

#include &lt;stddef.h&gt;
#include &lt;stdint.h&gt;

DLSYS_EXTERN_C &#123;
    typedef enum&#123;
        kCPU = 1,
        kGPU = 2,
    &#125;DLDeviceType;

    typedef struct&#123;
        int device_id;
        DLDeviceType device_type;
    &#125;DLContext;

    typedef struct&#123;
        void *data;
        DLContext ctx;
        int ndim;
        int64_t *shape;
    &#125;DLArray;
&#125;
</code></pre>
<h4 id="源码解析"><a href="#源码解析" class="headerlink" title="源码解析"></a>源码解析</h4><p>这段代码定义了最基本的数据结构。DLDeviceType采用枚举类型定义，定义此时系统选择的CPU还是GPU进行运算。DLContext用于定义运行时的CPU或GPU信息。DLArray定义数据结构Tensor，包含数据的指针、CPU或GPU信息、维度以及形状。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/CUDA-C%E4%B8%8EPython%E6%B7%B7%E5%90%88%E7%BC%96%E7%A8%8B%E6%80%9D%E8%B7%AF/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/CUDA-C%E4%B8%8EPython%E6%B7%B7%E5%90%88%E7%BC%96%E7%A8%8B%E6%80%9D%E8%B7%AF/" class="post-title-link" itemprop="url">CUDA/C与Python混合编程思路</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-15 18:06:22 / Modified: 18:35:24" itemprop="dateCreated datePublished" datetime="2022-10-15T18:06:22+08:00">2022-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/CUDA-Programming/" itemprop="url" rel="index"><span itemprop="name">CUDA Programming</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="CUDA-C-Python-混合编程思路"><a href="#CUDA-C-Python-混合编程思路" class="headerlink" title="CUDA C++ Python 混合编程思路"></a>CUDA C++ Python 混合编程思路</h1><h3 id="框架开发流程"><a href="#框架开发流程" class="headerlink" title="框架开发流程"></a>框架开发流程</h3><p><img src="/image/article/框架开发流程.png" alt="框架开发流程"><br>框架开发流程如上图所示。<br>为了明确框架开发的具体步骤，这里需要介绍一下相关的基础概念。</p>
<h5 id="动态链接库dll"><a href="#动态链接库dll" class="headerlink" title="动态链接库dll"></a>动态链接库dll</h5><p>动态链接库（Dynamic Link Library）是一个可以被其他应用程序共享的模块，其中封装了一些可以被共享的程序和模块。动态链接库是与静态链接库相对的一个概念。动态链接库代码并不可以单独执行，需要配合操作系统调用到指定的应用程序中才可以完成执行。</p>
<h3 id="1-使用VScode写CUDA代码"><a href="#1-使用VScode写CUDA代码" class="headerlink" title="1. 使用VScode写CUDA代码"></a>1. 使用VScode写CUDA代码</h3><p>创建cu文件和h头文件，在cu文件中撰写核函数和封装核函数的代码，如下图所示。<br><img src="/image/article/cudaK.png" alt="代码"></p>
<p><img src="/image/article/0EE6AB.png" alt="extern"><br>需要注意的是要将封装核函数在h文件中extern “C” 声明。</p>
<h3 id="2-用VScode终端进行编译调试"><a href="#2-用VScode终端进行编译调试" class="headerlink" title="2. 用VScode终端进行编译调试"></a>2. 用VScode终端进行编译调试</h3><p>a. 编译可执行文件命令</p>
<pre><code>nvcc -o hello-gpu hello-gpu.cu -run
</code></pre><p>b. 编译生成动态链接库文件.so</p>
<pre><code>nvcc -o hello-gpu.so -shared hello-gpu.cu
</code></pre><p>c. 创建一个main.cpp文件，包含hello-gpu.h之后，观察结果的命令</p>
<pre><code>g++ -o main main.cpp hello-gpu.so
</code></pre><h3 id="3-打开Visual-Studio-2019"><a href="#3-打开Visual-Studio-2019" class="headerlink" title="3. 打开Visual Studio 2019"></a>3. 打开Visual Studio 2019</h3><p>创建C++工程文件，把VScode中生成的hello-gpu.h，hello-gpu.so，hello-gpu.lib（三个文件缺一不可）放到Visual Studio的C++工程里。</p>
<p>写mian.c代码，头部加入</p>
<pre><code>#include &quot;hello-gpu.h&quot;
#pragma comment(lib, &quot;hello-gpu.lib&quot;)
</code></pre><p>并在头文件部分引入hello-gpu.h文件即可实现调用。</p>
<p>如果可以跑成功，但没有执行GPU部分的代码，考虑是GPU与CPU程序异步问题。解决方法是在cu文件的封装核函数代码内加入</p>
<pre><code class="lang-CUDA">cudaDeviceSynchronize();
</code></pre>
<h3 id="4-编写C-部分的文件，使用Pybind11生成pyd文件"><a href="#4-编写C-部分的文件，使用Pybind11生成pyd文件" class="headerlink" title="4. 编写C++部分的文件，使用Pybind11生成pyd文件"></a>4. 编写C++部分的文件，使用Pybind11生成pyd文件</h3><p>在VS2019上创建一个新的（也可以用旧的）C++工程，进入工程之后，切换到Release，x64模式。右键点击解决方案，进入属性界面。</p>
<p>a. 进入  常规 ——&gt; 配置类型，改为.dll动态库。<br><img src="/image/article/A804DF57EF797D0A137FEFA933BC5079.png" alt=""></p>
<p>b. 进入  高级 ——&gt; 目标文件扩展名，改为.pyd。<br><img src="/image/article/B1DD73F23611E506B651C9D2BB4AE3BA.png" alt=""></p>
<p>c. 进入  VC++目录，编辑包含目录，添加anaconda的include和pybind11的include路径。<br><img src="/image/article/C4ACFA4454B6518663D5441A239DC6D5.png" alt=""></p>
<p>d. 进入  VC++目录，编辑库目录，添加anaconda的libs路径。<br><img src="/image/article/3E4EB12CE8A3A07EA1EE2B38B267B0C7.png" alt=""></p>
<p>e. 进入  链接器 ——&gt; 输入 ——&gt; 添加依赖项，添加python39.lib，python3.lib。<br><img src="/image/article/A51CCCF89150FD2E40A0BAB512068FF9.png" alt=""></p>
<p>接下来按照pybind11的格式对编写好的C++程序进行封装。<br><img src="/image/article/9FD4023878CF02969256B626DB324E05.png" alt=""></p>
<h3 id="5-Python部分处理"><a href="#5-Python部分处理" class="headerlink" title="5. Python部分处理"></a>5. Python部分处理</h3><p>得到pyd文件之后，需要对文件进行重命名</p>
<pre><code>PYBIND11_MODULE(example, m)
</code></pre><p>将文件名和exmaple的名字保持一致。</p>
<p>同时，找到hello-gpu.so文件，放到当前的Python文件工程中文件夹下，在Python里就可以调用经过Pybind，C++封装的CUDA代码了。</p>
<p>生成的时候必须注意，需要将自己包含的头文件在VC++和链接器中加入<br><img src="/image/article/Pasted image 20220718210450.png" alt=""></p>
<p><img src="/image/article/Pasted image 20220718210502.png" alt=""></p>
<h3 id="VSCode-采用nvcc编译"><a href="#VSCode-采用nvcc编译" class="headerlink" title="VSCode 采用nvcc编译"></a>VSCode 采用nvcc编译</h3><ol>
<li><p>生成动态链接库</p>
<pre><code class="lang-nvcc">nvcc -o xxx.so --shared xxx.cu xxx.cu
</code></pre>
</li>
<li><p>携带相关头文件编译生成动态链接库</p>
<pre><code class="lang-nvcc">nvcc -o xxx.so --shared xxx.cu xxx.cu xxx.cu -L &quot;C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v11.3\lib\x64&quot; -l &quot;cudnn&quot;
</code></pre>
<p>-L : 表示需要去哪里寻找库文件<br>-l : 表示在-L指示下选择哪个文件（不需要加文件后缀）</p>
</li>
</ol>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91C%E8%BF%90%E8%A1%8C%E6%97%B6%E6%8E%A5%E5%8F%A3/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91C%E8%BF%90%E8%A1%8C%E6%97%B6%E6%8E%A5%E5%8F%A3/" class="post-title-link" itemprop="url">【Thunder源码阅读】Python数据结构封装</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-15 17:08:22 / Modified: 19:21:12" itemprop="dateCreated datePublished" datetime="2022-10-15T17:08:22+08:00">2022-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Source-Code-Reading-Note/" itemprop="url" rel="index"><span itemprop="name">Source Code Reading Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="C运行时接口"><a href="#C运行时接口" class="headerlink" title="C运行时接口"></a>C运行时接口</h1><blockquote>
<p>C Runtime API是将数据结构、设备管理、GPU加速等C++或CUDA代码综合封装，并统一暴露给Python的文件。在阅读本章之前需要了解的前置知识有：<a target="_blank" rel="noopener" href="https://blog.csdn.net/chenxiemin/article/details/110911324?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165892209016782391814254%2522%252C%2522scm%2522%253A%252220140713.130102334.pc%255Fall.%2522%257D&amp;request_id=165892209016782391814254&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~first_rank_ecpm_v1~rank_v31_ecpm-3-110911324-null-null.142^v35^control,185^v2^control&amp;utm_term=C%2B%2B%20static%20cast&amp;spm=1018.2226.3001.4187">Static Cast</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/itlilyer/article/details/107561110?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165892168616782350851615%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165892168616782350851615&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~top_click~default-1-107561110-null-null.142^v35^control,185^v2^control&amp;utm_term=C%2B%2B%20array&amp;spm=1018.2226.3001.4187">Array容器</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/liu16659/article/details/87152348?ops_request_misc=%257B%2522request%255Fid%2522%253A%2522165892182516781683943730%2522%252C%2522scm%2522%253A%252220140713.130102334..%2522%257D&amp;request_id=165892182516781683943730&amp;biz_id=0&amp;utm_medium=distribute.pc_search_result.none-task-blog-2~all~baidu_landing_v2~default-1-87152348-null-null.142^v35^control,185^v2^control&amp;utm_term=C%2B%2B%20fill&amp;spm=1018.2226.3001.4187">fill</a>，<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36614557/article/details/119283085?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_utm_term~default-0-119283085-blog-80609369.pc_relevant_default&amp;spm=1001.2101.3001.4242.1&amp;utm_relevant_index=3">子类向父类的转换</a>。</p>
</blockquote>
<h2 id="c-runtime-api-h-文件源码"><a href="#c-runtime-api-h-文件源码" class="headerlink" title="c_runtime_api.h 文件源码"></a>c_runtime_api.h 文件源码</h2><pre><code class="lang-Cpp">#ifndef DLSYS_RUNTIME_C_RUNTIME_API_H_
#define DLSYS_RUNTIME_C_RUNTIME_API_H_

#ifdef __cplusplus
#define DLSYS_EXTERN_C extern &quot;C&quot;
#else
#define DLSYS_EXTERN_C
#endif

#include &quot;dlarray.h&quot;
#include &lt;stddef.h&gt;
#include &lt;stdint.h&gt;

DLSYS_EXTERN_C &#123;

    typedef int64_t index_t;
    typedef DLArray *DLArrayHandle;
    typedef void *DLStreamHandle;

    int DLArrayAlloc(const index_t *shape, index_t ndim, DLContext ctx,
    DLArrayHandle *out);

    int DLArrayFree(DLArrayHandle handle);

    int DLArrayCopyFromTo(DLArrayHandle from, DLArrayHandle to,    
    DLStreamHandle stream);
    ---------------------------------------------------------------------------
    int DLGpuArraySet(DLArrayHandle arr, float value);

    int DLArrayReshape(const DLArrayHandle handle, const index_t *new_shape, index_t new_dim);

    int DLGpuBroadcastTo(const DLArrayHandle input, DLArrayHandle output);

    int DLGpuReduceSumAxisZero(const DLArrayHandle input, DLArrayHandle output);

    int DLGpuMatrixElementwiseAdd(const DLArrayHandle matA,
    const DLArrayHandle matB, DLArrayHandle output);

    int DLGpuMatrixElementwiseAddByConst(const DLArrayHandle input, float val,
    DLArrayHandle output);

    int DLGpuMatrixElementwiseSubtract(const DLArrayHandle matA,
    const DLArrayHandle matB, DLArrayHandle output);

    int DLGpuMatrixElementwiseSubtractByConst(const DLArrayHandle input, float val,
    DLArrayHandle output);

    int DLGpuMatrixElementwiseMultiply(
    const DLArrayHandle matA, const DLArrayHandle matB, DLArrayHandle output);

    int DLGpuMatrixMultiplyByConst(const DLArrayHandle input, float val,
    DLArrayHandle output);

    int DLGpuMatrixElementwiseDivByConst(const DLArrayHandle matA, float val,    
    DLArrayHandle output);  

    int DLGpuMatrixMultiply(const DLArrayHandle matA, bool transposeA,    
    const DLArrayHandle matB, bool transposeB,
    DLArrayHandle matC);

    int DLGpuRelu(const DLArrayHandle input, DLArrayHandle output);

    int DLGpuReluGradient(const DLArrayHandle input, const DLArrayHandle in_grad,
    DLArrayHandle output);

    int DLGpuSoftmax(const DLArrayHandle input, DLArrayHandle output);

    int DLGpuSoftmaxCrossEntropy(const DLArrayHandle input_a,
    const DLArrayHandle input_b,
    DLArrayHandle output); 

    int DLGpuMatrixElementwiseSqrt(const DLArrayHandle input_a, DLArrayHandle output);

&#125;

#endif
</code></pre>
<p>——下面的代码是直接调用GPU封装好的计算函数的，这里是为了暴露给Python而定义的。</p>
<h2 id="c-runtime-api-cpp源码解析"><a href="#c-runtime-api-cpp源码解析" class="headerlink" title="c_runtime_api.cpp源码解析"></a>c_runtime_api.cpp源码解析</h2><p>首先在dlsys::runtime的命名空间之下，定义一个管理器DeviceAPIManager。源代码如下：</p>
<pre><code class="lang-Cpp">class DeviceAPIManager &#123;

    public:
        static const int kMaxDeviceAPI = 8;

        static DeviceAPI *Get(DLContext ctx) &#123;
            return Global()-&gt;GetAPI(ctx.device_type);
        &#125;

    private:
        std::array&lt;DeviceAPI *, kMaxDeviceAPI&gt; api_;

        DeviceAPIManager() &#123;
            std::fill(api_.begin(), api_.end(), nullptr);
            static CPUDeviceAPI cpu_device_api_inst;    
            static CUDADeviceAPI gpu_device_api_inst;
            api_[kCPU] = static_cast&lt;DeviceAPI *&gt;(&amp;cpu_device_api_inst);
            api_[kGPU] = static_cast&lt;DeviceAPI *&gt;(&amp;gpu_device_api_inst);

        &#125;
        // Get global static variable.
        static DeviceAPIManager *Global() &#123;
            static DeviceAPIManager inst;
            return &amp;inst;
        &#125;
        // Get API.
        DeviceAPI *GetAPI(DLDeviceType type) &#123;        
            if (api_[type] == nullptr) &#123;
                std::cerr &lt;&lt; &quot;Device API not supported&quot; &lt;&lt; std::endl;
                exit(EXIT_FAILURE);
        &#125;        
        return api_[type];
    &#125;
&#125;;
</code></pre>
<p>kMaxDeviceAPI定义的是最大的设备数量。<br>在private中，首先声明一个数组容器，用来存放类型为DeviceAPI的kMax个变量。我们知道DeviceAPI是对运算设备行为的抽象，因此这里的意思就是为了统一管理运算设备，建立了有kMax个凹槽的容器。</p>
<p>private中，DeviceAPIManager是构造函数，首先用空指针填充api_容器，这是对数据的初始化。进一步创建了CPU和GPU两个设备的实例化对象。由于这两个所属的类是子类，为了符合api的统一管理需求，同时为了不丧失子类的特性，这里采用<a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36614557/article/details/119283085?utm_medium=distribute.pc_relevant.none-task-blog-2~default~baidujs_utm_term~default-0-119283085-blog-80609369.pc_relevant_default&amp;spm=1001.2101.3001.4242.1&amp;utm_relevant_index=3">子类向父类的转换</a>。</p>
<p>private中，Global函数是创建了一个DeviceAPIManager，即创建了一个管理器，返回管理器的地址。这个管理器是static的，所以在一个程序周期内仅仅拥有这一个管理器。</p>
<p>public中，Get函数的输入是DLContext，详情见[[GPU运算数据结构]]（DLContext里面包含了device_type变量，用于表示现在是用哪个设备进行运算）。先调用Global获取管理器，管理器的一个成员函数是GetAPI。</p>
<p>private中，GetAPI用于从初始化好的api_容器里找到对应的设备管理器的指针，并返回。这个设备管理器具有[[设备管理]]CPU和GPU的对应的方法。可以通过‘-&gt;’进行调用。</p>
<pre><code class="lang-Cpp">inline DLArray *DLArrayCreate_() &#123;

    DLArray *arr = new DLArray();

    arr-&gt;shape = nullptr;

    arr-&gt;ndim = 0;

    arr-&gt;data = nullptr;

    return arr;

&#125;



inline void DLArrayFree_(DLArray *arr) &#123;

    if (arr != nullptr) &#123;

        // ok to delete nullptr

        delete[] arr-&gt;shape;

        if (arr-&gt;data != nullptr) &#123;

            DeviceAPIManager::Get(arr-&gt;ctx)-&gt;FreeDataSpace(arr-&gt;ctx, arr-&gt;data);

        &#125;

    &#125;

    delete arr;

&#125;



inline size_t GetDataSize(DLArray *arr) &#123;

    size_t size = 1;

    for (index_t i = 0; i &lt; arr-&gt;ndim; ++i) &#123;

        size *= arr-&gt;shape[i];

    &#125;

    // assume 32-bit float

    size *= 4;

    return size;

&#125;



inline size_t GetDataAlignment(DLArray *arr) &#123;

// assume 32-bit float

    return 8;

&#125;
</code></pre>
<p>四个inline内联函数定义在dlsys::runtime下。<br>DLArrayCreate_实现数组的创建和初始化。<br>DLArrayFree_实现数组的释放。<br>GetDataSize用于计算数组的总数量，比如(3,256,256)shape的数据，会计算size是3<br>x256x256。当然这只是数据的数量，代码中假设数据是32bit的float，由于size表示的是字节数的意思，所以算出来的size要乘4。<br>GetDataAlignment（待完成）。</p>
<p>接下来是封装函数，暴露接口给Python的工作。</p>
<pre><code class="lang-Cpp">using namespace dlsys::runtime;



int DLArrayAlloc(const index_t *shape, index_t ndim, DLContext ctx,

    DLArrayHandle *out) &#123;

    DLArray *arr = nullptr;
    API_BEGIN() ;
    // shape
    arr = DLArrayCreate_();
    // ndim
    arr-&gt;ndim = ndim;
    index_t *shape_copy = new index_t[ndim];
    std::copy(shape, shape + ndim, shape_copy);
    arr-&gt;shape = shape_copy;
    // ctx
    arr-&gt;ctx = ctx;
    size_t size = GetDataSize(arr);
    size_t alignment = GetDataAlignment(arr);
    arr-&gt;data = DeviceAPIManager::Get(ctx)-&gt;AllocDataSpace(ctx, size, alignment);
    *out = arr;
    API_END_HANDLE_ERROR(DLArrayFree_(arr));

&#125;
</code></pre>
<p>上述代码实现了DLArray的创建和初始化。</p>
<pre><code class="lang-Cpp">int DLArrayFree(DLArrayHandle handle) &#123;

    API_BEGIN() ;

    DLArray *arr = handle;

    DLArrayFree_(arr);

    API_END();

&#125;



int DLArrayReshape(const DLArrayHandle handle, const index_t *new_shape, index_t new_dim) &#123;

    API_BEGIN() ;

    DLArray *arr = handle;

    index_t *shape_copy = new index_t[new_dim];

    std::copy(new_shape, new_shape + new_dim, shape_copy);

    arr-&gt;shape = shape_copy;

    arr-&gt;ndim = new_dim;

    API_END();

&#125;



int DLArrayCopyFromTo(DLArrayHandle from, DLArrayHandle to,

DLStreamHandle stream) &#123;

    API_BEGIN() ;

    size_t from_size = GetDataSize(from);

    size_t to_size = GetDataSize(to);

    // The size must exactly match

    assert(from_size == to_size);

    DLContext ctx = from-&gt;ctx;

    if (ctx.device_type == kCPU) &#123;

        ctx = to-&gt;ctx;

    &#125; 
    else &#123;

        // Can not copy across different ctx types directly

        assert((to-&gt;ctx.device_type == kCPU) ||

        (to-&gt;ctx.device_type == from-&gt;ctx.device_type));

    &#125;

    DeviceAPIManager::Get(ctx)-&gt;CopyDataFromTo(from-&gt;data, to-&gt;data, from_size,

    from-&gt;ctx, to-&gt;ctx, stream);

    API_END();

&#125;
</code></pre>
<p>DLArrayFree用于释放内存。<br>DLArrayReshape用于改变数组的shape。<br>DLArrayFromTo用于实现数据的搬运。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/%E3%80%90%E5%8A%A8%E6%89%8B%E5%BC%80%E5%8F%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%97%A5%E8%AE%B0%E3%80%91Tensor%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/%E3%80%90%E5%8A%A8%E6%89%8B%E5%BC%80%E5%8F%91%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E6%A1%86%E6%9E%B6%E6%97%A5%E8%AE%B0%E3%80%91Tensor%E5%9F%BA%E6%9C%AC%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84/" class="post-title-link" itemprop="url">【动手开发深度学习框架日记】Tensor基本数据结构</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-15 17:04:25 / Modified: 17:57:52" itemprop="dateCreated datePublished" datetime="2022-10-15T17:04:25+08:00">2022-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Neutron/" itemprop="url" rel="index"><span itemprop="name">Neutron</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <blockquote>
<p>我的专业是智能科学与技术专业，硬件软件方面的知识都有涉及一些。深度学习技术在我学习生涯中占比例是很大的。虽然学习过一些机器学习、神经网络等一些理论基础，但是用别人成熟的框架总感觉缺失点东西。以学习出发为目的，我花了一两个月时间使用Numpy+Python写了第一个自己的深度学习框架MetaFlow，实现了Tensor、反向传播、自动微分算子、Dataset、DataLoader等模块，使用起来类似于Pytorch，但是由于学习种种原因就被搁置了，仅仅实现了全连接神经网络的封装。同时MetaFlow还有一个缺点就是没有使用GPU加速。CUDA程序设计也是我比较期待学习的。所以产生了写一个基于CUDA/C为后端，Python为前端的简单的深度学习框架。这次的目标主要是实现全连接以及卷积操作，并提供类似于Pytorch的接口，同时支持batch训练、模型保存等等功能。<br>这个系列的博客用于个人学习记录，同时把一些实现的方法分享出来，也算是项目的参考文档。</p>
</blockquote>
<h1 id="一、数据结构的定义"><a href="#一、数据结构的定义" class="headerlink" title="一、数据结构的定义"></a>一、数据结构的定义</h1><p><strong>张量</strong>（<strong>Tensor</strong>）是在任何深度学习框架中最为重要的一个数据结构。该数据结构需要实现以下几个功能：</p>
<blockquote>
<ul>
<li><strong>支持高维度矩阵的运算</strong></li>
<li><strong>允许记录梯度值，且可以设置是否需要求梯度</strong></li>
<li><strong>作为计算图中的结点可以记录父节点和子节点</strong></li>
<li><strong>可以记录得到该结点时运用了哪些算子</strong></li>
<li><strong>能够做到数据从CPU（host）到GPU（device）的转换和创建</strong></li>
<li><strong>支持反向传播算法</strong></li>
</ul>
</blockquote>
<p>这里需要关注的就是CPU到GPU数据的转换和格式如何去定义。</p>
<h4 id="1-1-CPU数据结构和GPU数据结构"><a href="#1-1-CPU数据结构和GPU数据结构" class="headerlink" title="1.1 CPU数据结构和GPU数据结构"></a>1.1 CPU数据结构和GPU数据结构</h4><p>我采取了两种基本数据类型，分别时Numpy数组（CPU端计算）以及Quark（GPU端计算），Tensor在这里的作用更像是管理者和资源记录分配者，而真正需要进行数据运算的是上面二位基本数据结构支持的。Quark也叫夸克（项目名称是Neutron，中文是中子的意思，为了战术上的统一就给后端的数据结构起了名字叫夸克），是定义在后端的结构体。具体实现的代码如下：</p>
<pre><code class="lang-cpp">extern &quot;C&quot;&#123;
    typedef enum&#123;
        CPU = 0,
        GPU = 1
    &#125;Device;

    typedef struct&#123;
        float* data;
        Device device;
        int* shape;
        int dim;
    &#125;Quark;
&#125;
</code></pre>
<p>代码定义在 _array.h_ 头文件当中，其中包含float型数据指针；enum类型Device判断是使用CPU计算数据还是GPU；同样是数组指针shape，用于记录张量形状；以及dim记录张量维数。这些是在CUDA程序中十分重要的几个参数。</p>
<p>为了能够使C/C++的API供Python调用，使用了Python内置的ctypes库。那么在Python端，将CUDA/C++编译好的动态链接库文件导入，就可以使用后端的数据结构和API了。关于混合编程以及接口调用的问题，会在另一个文章里记录说明（<strong>未更新</strong>）。</p>
<p>总之，Quark结构体映射至Python代码中的实现如下：</p>
<pre><code class="lang-python">class Quark(ctypes.Structure):
    &quot;&quot;&quot;
        C++ back-end data structure. Contains data pointer (numpy data type has to
        be float32, otherwise it&#39;ll raise calculate error), device, data shape poin
        ter and dimension.
    &quot;&quot;&quot;
    _fields_ = [(&#39;data&#39;, ctypes.POINTER(c_float)),
                (&#39;device&#39;, ctypes.c_int),
                (&#39;shape&#39;, ctypes.POINTER(ctypes.c_int)),
                (&#39;dim&#39;, ctypes.c_int)]
</code></pre>
<p>关于使用CPU计算的数据就很简单了，np.array()就完事儿，什么shape，dim都能获取到。</p>
<h4 id="1-2-Tensor定义"><a href="#1-2-Tensor定义" class="headerlink" title="1.2 Tensor定义"></a>1.2 Tensor定义</h4><p>之前提到过，Tensor在框架中的角色其实并不是计算，而是充当一个资源调度和管理的角色。那么它就要能集两家之数据（CPU和GPU），无缝的、信息不丢失的衔接切换。同时还要实现什么梯度记录呀、父子结点记录呀、反向传播算法等等功能。当然要实现数据到GPU还是需要一些CUDA代码的，本篇日记仅仅记录实现逻辑，背后的CUDA代码会在另一个记录中说明（<strong>未更新</strong>）。</p>
<p>首先定义一个类，Tensor类，类中的属性就可以按照我们的需求来</p>
<pre><code class="lang-python">class Tensor:
    &quot;&quot;&quot;
        Python fore-end data structure.
        The most important attr is handle. Handle is a pointer to the real data str
        -ucture. It manages GPU data structure (Quark) and CPU data structure (numpy).

        When you instantiate the Tensor, you need to give parameters as follows:
        1. data: numpy array, dtype is np.float32.
        2. device: on cpu or on gpu.
        3. require_grad: require calculate gradient or not.
    &quot;&quot;&quot;

    def __init__(self, data, device=CPU, require_grad=False):
        self.children = []
        self.father = []
        self.op = None
        self.grad = None
        self.device = device
        self.require_grad = require_grad
        self.handle = self.configureHandle(self, data, device)
</code></pre>
<p>里面包括父子结点列表、op算子、梯度、设备（CPU/GPU）、求梯度标志位、数据结构句柄。这个handle的作用就是根据device属性来判别需要创建什么句柄。</p>
<p>这里涉及到类的函数，<code>self.configureHandle()</code>，实现如下：</p>
<pre><code class="lang-python">    @staticmethod
    # configure the handle attribute
    def configureHandle(self, data, device):
        if isinstance(data, tuple):
            data = np.random.random(data)
        if device == GPU:
            return self.getQuarkHandle(data.astype(np.float32))
        elif device == CPU:
            return self.getNumpyHandle(data.astype(np.float32))

    @staticmethod
    # get the Quark data structure handle
    def getQuarkHandle(numpy_data):
        assert isinstance(numpy_data, ndarray), &quot;input data should be numpy array&quot;
        data = numpy_data
        arr = Quark()
        arr.data = data.ctypes.data_as(ctypes.POINTER(c_float))
        arr.device = GPU
        arr.shape = getShape(ctypes.c_int, data.shape)
        arr.dim = len(data.shape)

        # start to allocate and copy data to GPU
        size = CUDALib.getSize(arr.dim, arr.shape)
        dev_ptr = CUDALib.AllocateDeviceData(size)
        CUDALib.CopyDataFromTo(arr.data, dev_ptr, CPU, GPU, size)
        arr.data = dev_ptr
        return arr

    @staticmethod
    # get the numpy data structure handle4
    def getNumpyHandle(numpy_data):
        assert isinstance(numpy_data, ndarray), &quot;input data should be numpy array&quot;
        return numpy_data
</code></pre>
<pre><code>首先判断要在CPU计算还是GPU计算，分别转到`getNumpyHandle()`和`getQuarkHandle()`。第一个函数实现很简单，返回numpy数组即可。`getQuarkHandle()`首先要从numpy数据中获取想要的信息，实例化Quark，在根据Quark的信息调用后端实现CUDA内存分配代码，把数据直接加载到GPU显存上。最后返回Quark实例化数据。

为了方便观察和调试，需要提供一些数据获取的接口。

```python
    @property
    def shape(self):  # get data shape
        if isinstance(self.handle, ndarray):
            return self.handle.shape
        return tuple([self.handle.shape[idx] for idx in range(self.handle.dim)])

    @property
    def data(self):  # get data
        assert(self.device == GPU), &quot;the data on the gpu instead of cpu&quot;
        return np.ctypeslib.as_array(self.handle.data, shape=self.shape)

    def __str__(self):

        return &quot;Tensor(&#123;&#125;, shape=&#123;&#125;, dtype=Tensor.float32)&quot;.format(np.ctypeslib.as_array(self.handle.data, shape=self.shape), self.shape)
</code></pre><p>和Pytorch类似，转移Tensor数据时，只需要xx.cpu()或者xx.gpu()即可。具体代码如下：</p>
<pre><code class="lang-python">    # transfer the data from the gpu to the cpu
    def cpu(self):
        if self.device == GPU:
            size = CUDALib.getSize(self.handle.dim, self.handle.shape)
            host_ptr = CUDALib.AllocateHostData(size)
            CUDALib.CopyDataFromTo(self.handle.data, host_ptr, GPU, CPU, size)
            self.handle.data = host_ptr
        return self

    # transfer the data from the cpu to the gpu
    def gpu(self):
        if self.device == CPU and isinstance(self.handle, ndarray):
            self.handle = self.getQuarkHandle(self.handle)
            self.device = GPU
        return self
</code></pre>
<p><code>gpu()</code>实现的逻辑就调用了<code>getQuarkHandle()</code>创建一个GPU的Quark。<code>cpu()</code>则需要利用CUDA的API将GPU显存数据移动到CPU上。</p>
<p>对于反向传播方法和剩余的方法将分布在其他文章中解释（<strong>未更新</strong>）。</p>
<h1 id="二、使用例程"><a href="#二、使用例程" class="headerlink" title="二、使用例程"></a>二、使用例程</h1><p>使用Tensor，可以预先创建numpy数组，前提 <strong>必须</strong> 数据是 <strong>np.float32</strong> 类型。这是因为在后端定义Quark是float类型，如果采用其他数据类型GPU运算出的数据会千奇百怪（如果忘记也没有关系，代码中会自动将类型变为float32）。<br>首先，可以使用numpy创建任意维度的数组，然后通过Tensor进一步封装。</p>
<pre><code class="lang-python">x = np.ones((64, 64))
xt = Tensor(x, CPU, require_grad=False)
print(xt)

# console results
Tensor([[1. 1. 1. ... 1. 1. 1.]
 [1. 1. 1. ... 1. 1. 1.]       
 [1. 1. 1. ... 1. 1. 1.]       
 ...
 [1. 1. 1. ... 1. 1. 1.]
 [1. 1. 1. ... 1. 1. 1.]
 [1. 1. 1. ... 1. 1. 1.]], shape=(64, 64), dtype=Tensor.float32)
</code></pre>
<p>我们可以通过调用<code>cpu()</code>和<code>gpu()</code>转移Tensor。</p>
<pre><code class="lang-python">xt.cpu()
xt.gpu()
</code></pre>
<p>值得注意的是，若想要打印数据，必须调用<code>cpu()</code>把数据移动到CPU来。<br>也可以通过输入元组数据，Tensor会自动创建基于正态分布的随机张量。</p>
<pre><code class="lang-python">xt = Tensor((1, 64, 64), CPU, require_grad=False)
print(xt)

# console results
Tensor([[[0.43749017 0.29031968 0.8365907  ... 0.61214393 0.44423762 0.03210686]
  [0.6642815  0.7885864  0.6017005  ... 0.28682867 0.49431917 0.64389694]
  [0.02547996 0.5165705  0.711713   ... 0.33360547 0.13552403 0.6047031 ]
  ...
  [0.5312942  0.13073258 0.39996797 ... 0.3393874  0.38398758 0.81480604]
  [0.08465459 0.855784   0.6820476  ... 0.10212806 0.11926474 0.6199378 ]
  [0.92551076 0.92917097 0.8674459  ... 0.34977752 0.55820996 0.50206757]]], shape=(1, 64, 64), dtype=Tensor.float32)
</code></pre>
<p>以上就是目前数据结构的全部内容啦，当然会随着开发的完善逐步更新这个博客。感谢阅读！</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%B0%81%E8%A3%85/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2022/10/15/%E3%80%90Thunder%E6%BA%90%E7%A0%81%E9%98%85%E8%AF%BB%E3%80%91Python%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E5%B0%81%E8%A3%85/" class="post-title-link" itemprop="url">【Thunder源码阅读】Python数据结构封装</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-15 10:06:22 / Modified: 19:22:12" itemprop="dateCreated datePublished" datetime="2022-10-15T10:06:22+08:00">2022-10-15</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Source-Code-Reading-Note/" itemprop="url" rel="index"><span itemprop="name">Source Code Reading Note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="Python-数据结构封装"><a href="#Python-数据结构封装" class="headerlink" title="Python 数据结构封装"></a>Python 数据结构封装</h1><blockquote>
<p>ndarray文件夹实现了python与[[C运行时接口]]的对接。这一块的数据结构是专门用来实现GPU数据的进一步封装的。<br>_base 文件是用来加载动态连结库的，并提供了一些实用的小函数。<br>gpu_op是对CUDA内核的进一步封装。<br>ndarray则是对DLArray的进一步封装。<br>前两个文件的思路较为简单，这里直接对ndarray文件进行分析。</p>
</blockquote>
<h2 id="ndarray-py源码极其分析"><a href="#ndarray-py源码极其分析" class="headerlink" title="ndarray.py源码极其分析"></a>ndarray.py源码极其分析</h2><pre><code class="lang-python">from __future__ import absolute_import
from ._base import _LIB, check_call, c_array
from . import ndarray as _nd

import ctypes
import numpy as np

class DLContext(ctypes.Structure):

    _fields_ = [(&quot;device_id&quot;, ctypes.c_int),
    (&quot;device_type&quot;, ctypes.c_int)]

    MASK2STR = &#123;
    1: &#39;cpu&#39;,
    2: &#39;gpu&#39;,
    &#125;


    def __init__(self, device_id, device_type):
        super(DLContext, self).__init__()
        self.device_id = device_id
        self.device_type = device_type


    def __repr__(self):
        return &quot;%s(%d)&quot; % (DLContext.MASK2STR[self.device_type], self.device_id)
</code></pre>
<p>这里实现的是对DLContext的封装，这是ctype所必须的操作。</p>
<pre><code class="lang-python">class DLArray(ctypes.Structure):

    _fields_ = [(&quot;data&quot;, ctypes.c_void_p),

    (&quot;ctx&quot;, DLContext),

    (&quot;ndim&quot;, ctypes.c_int),

    (&quot;shape&quot;, ctypes.POINTER(ctypes.c_int64))]

# 指针类型

DLArrayHandle = ctypes.POINTER(DLArray)

def cpu(dev_id=0):

    return DLContext(dev_id, 1)

def gpu(dev_id=0):

    return DLContext(dev_id, 2)

def is_gpu_ctx(ctx):

    return ctx and ctx.device_type == 2
</code></pre>
<p>对DLArray的封装，并提供了指针和一些小函数。</p>
<pre><code class="lang-python">def array(arr, ctx=cpu(0)):

    if not isinstance(arr, np.ndarray):

    arr = np.array(arr)

    ret = empty(arr.shape, ctx)

    ret._sync_copyfrom(arr)

    return ret




def empty(shape, ctx=cpu(0)):

    shape = c_array(ctypes.c_int64, shape)

    ndim = ctypes.c_int(len(shape))

    handle = DLArrayHandle()

    check_call(_LIB.DLArrayAlloc(

    shape, ndim, ctx, ctypes.byref(handle)))

    return NDArray(handle)




def reshape(arr, new_shape):

    assert isinstance(arr, _nd.NDArray)

    shape = c_array(ctypes.c_int64, new_shape)

    new_dim = len(new_shape)

    handle = arr.handle

    check_call(_LIB.DLArrayReshape(handle, shape, new_dim))
</code></pre>
<p>empty函数根据shape，会在cpu上创建一个DLArray，分配对应的内存空间，并返回。<br>array则是将非numpy array类型的数据转换为numpy。<br>reshape则调用LIB中的函数，改变数据的形状。</p>
<pre><code class="lang-python">class NDArray(object):
    __slots__ = [&#39;handle&#39;]

    def __init__(self, handle):
        self.handle = handle


    def __del__(self):
        check_call(_LIB.DLArrayFree(self.handle))


    @property
    def shape(self):
        return tuple(self.handle.contents.shape[i]
                        for i in range(self.handle.contents.ndim))

    @property
    def ctx(self):
        return self.handle.contents.ctx



    def __setitem__(self, in_slice, value):
        &quot;&quot;&quot;Set ndarray value&quot;&quot;&quot;
        if (not isinstance(in_slice, slice) or
                                    in_slice.start is not None
                                    or in_slice.stop is not None):
            raise ValueError(&#39;Array only support set from numpy array&#39;)
        if isinstance(value, NDArray):
            if value.handle is not self.handle:
                value.copyto(self)
        elif isinstance(value, (np.ndarray, np.generic)):
            self._sync_copyfrom(value)
        else:
            raise TypeError(&#39;type %s not supported&#39; % str(type(value)))



    def _sync_copyfrom(self, source_array):
        if not isinstance(source_array, np.ndarray):
            try:
                source_array = np.array(source_array, dtype=np.float32)
            except:
                raise TypeError(&#39;array must be an array_like data,&#39; +
        &#39;type %s is not supported&#39;
                            % str(type(source_array)))
        source_array = np.ascontiguousarray(source_array, dtype=np.float32)
        if source_array.shape != self.shape:
            raise ValueError(&#39;array shape do not match the shape of NDArray&#39;)
        source_arr, shape = NDArray._numpyasarray(source_array)
        check_call(_LIB.DLArrayCopyFromTo(
                        ctypes.byref(source_arr), self.handle, None))
        # de-allocate shape until now
        _ = shape



    @staticmethod

    def _numpyasarray(np_data):
        data = np_data
        assert data.flags[&#39;C_CONTIGUOUS&#39;]
        arr = DLArray()
        shape = c_array(ctypes.c_int64, data.shape)
        arr.data = data.ctypes.data_as(ctypes.c_void_p)
        arr.shape = shape
        arr.ndim = data.ndim
        arr.ctx = cpu(0)
        return arr, shape



    def asnumpy(self):
        np_arr = np.empty(self.shape, dtype=np.float32)
        arr, shape = NDArray._numpyasarray(np_arr)
        check_call(_LIB.DLArrayCopyFromTo(
        self.handle, ctypes.byref(arr), None))
        _ = shape
        return np_arr



    def copyto(self, target):
        if isinstance(target, DLContext):
            target = empty(self.shape, target)
        if isinstance(target, NDArray):
            check_call(_LIB.DLArrayCopyFromTo(
                    self.handle, target.handle, None))
        else:
            raise ValueError(&quot;Unsupported target type %s&quot; % str(type(target)))
        return target
</code></pre>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
