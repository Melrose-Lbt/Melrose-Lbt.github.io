<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 6.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">


<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"melrose-lbt.github.io","root":"/","scheme":"Pisces","version":"7.8.0","exturl":false,"sidebar":{"position":"left","display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta name="description" content="【Paper Reading Note】Reducing the dimensionality of data with neural networks[TOC] Author: G.E. Hinton and R.R. Salakhutdinov  Abstract: High-dimensional data can be converted to low-dimensional codes">
<meta property="og:type" content="article">
<meta property="og:title" content="【Paper Reading Note】Reducing the dimensionality of data with neural networks">
<meta property="og:url" content="https://melrose-lbt.github.io/2022/10/20/Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks/index.html">
<meta property="og:site_name" content="Hexo">
<meta property="og:description" content="【Paper Reading Note】Reducing the dimensionality of data with neural networks[TOC] Author: G.E. Hinton and R.R. Salakhutdinov  Abstract: High-dimensional data can be converted to low-dimensional codes">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://melrose-lbt.github.io/image/article/RBM.png">
<meta property="og:image" content="https://melrose-lbt.github.io/image/article/MaxLike.jpeg">
<meta property="og:image" content="https://melrose-lbt.github.io/image/article/encoder.png">
<meta property="article:published_time" content="2022-10-20T11:23:22.000Z">
<meta property="article:modified_time" content="2022-10-20T12:07:44.000Z">
<meta property="article:author" content="John Doe">
<meta property="article:tag" content="Paper">
<meta property="article:tag" content="Deep Belief Network">
<meta property="article:tag" content="Boltzman machine">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="https://melrose-lbt.github.io/image/article/RBM.png">

<link rel="canonical" href="https://melrose-lbt.github.io/2022/10/20/Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : false,
    isPost : true,
    lang   : 'en'
  };
</script>

  <title>【Paper Reading Note】Reducing the dimensionality of data with neural networks | Hexo</title>
  






  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="Toggle navigation bar">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">Hexo</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>Home</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>Archives</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content post posts-expand">
            

    
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="en">
    <link itemprop="mainEntityOfPage" href="https://melrose-lbt.github.io/2022/10/20/Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="John Doe">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Hexo">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">
          【Paper Reading Note】Reducing the dimensionality of data with neural networks
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">Posted on</span>
              

              <time title="Created: 2022-10-20 19:23:22 / Modified: 20:07:44" itemprop="dateCreated datePublished" datetime="2022-10-20T19:23:22+08:00">2022-10-20</time>
            </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">In</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/Paper-reading-note/" itemprop="url" rel="index"><span itemprop="name">Paper reading note</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h1 id="【Paper-Reading-Note】Reducing-the-dimensionality-of-data-with-neural-networks"><a href="#【Paper-Reading-Note】Reducing-the-dimensionality-of-data-with-neural-networks" class="headerlink" title="【Paper Reading Note】Reducing the dimensionality of data with neural networks"></a>【Paper Reading Note】Reducing the dimensionality of data with neural networks</h1><p>[TOC]</p>
<p><strong>Author</strong>: <strong>G.E. Hinton and R.R. Salakhutdinov</strong></p>
<blockquote>
<p><strong>Abstract</strong>: High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “auto encoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep auto encoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.</p>
</blockquote>
<h2 id="1-Background-Knowledge"><a href="#1-Background-Knowledge" class="headerlink" title="_1. Background Knowledge_"></a><strong>_1. Background Knowledge_</strong></h2><h3 id="1-1-Boltzman-machine-amp-Restricted-Boltzman-machine"><a href="#1-1-Boltzman-machine-amp-Restricted-Boltzman-machine" class="headerlink" title="_1.1 Boltzman machine &amp; Restricted Boltzman machine_"></a><strong>_1.1 Boltzman machine &amp; Restricted Boltzman machine_</strong></h3><p>The first problem is what is the Boltzman machine. A <strong>Boltzmann machine</strong> is a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off. A <strong>Restricted Boltzman machine</strong> is a network like the two layers of fully connected layer. The visualization of two machines are shown in <strong>Fig. 1</strong>.</p>
<p><img src="/image/article/RBM.png" alt=""></p>
<center>Fig.1 Visualization of BM and RBM.</center>

<p>In this article, we mainly focus on the <strong>Restricted Boltzman machine</strong> (<strong>RBM</strong> in short).</p>
<h4 id="Mechanism-of-RBM"><a href="#Mechanism-of-RBM" class="headerlink" title="_Mechanism of RBM_"></a><strong>_Mechanism of RBM_</strong></h4><p>The <strong>Restricted Boltzman machine</strong> has two layer. The first layer is the visible layer, and the second layer is the hidden layer. Each layer has several nodes. Each node has two states, 1 or 0. We simply called these layer $v_i$, $h_j$, which means the ith node for the visible layer and the jth node for the hidden layer. The weight between the node $v_i$ and $h_j$ is $w_{ij}$. Each node also has bias.</p>
<p>The total energy for the given state vectors $\vec{v},\vec{h}$ is :</p>
<script type="math/tex; mode=display">
E(\vec{v}, \vec{h})= -\sum_ia_iv_i-\sum_jb_jh_j-\sum_{i,j}v_ih_jw_{ij}</script><p><strong>RBM</strong> is a probability-based model. In each time step, <strong>RBM</strong> has a certain state which can be descriped as $(\vec{v}, \vec{h})$. The probability distribution is :</p>
<script type="math/tex; mode=display">
p(\vec{v}, \vec{h})=\frac{e^{-E(\vec{v}, \vec{h} ) } }{\sum_{\vec{v}, \vec{h}}e^{-E(\vec{v}, \vec{h})}}</script><p>This is so called <strong>Boltzman distribution</strong>. It gives the probability when the particle in a certain state.</p>
<p>However, to calculate the (2) distribution is difficult. But to calculate the conditional distribution is relatively easier.</p>
<script type="math/tex; mode=display">
p(\vec{h}|\vec{v})=\prod_ip(h_i|\vec{v}) \\
p(\vec{v}|\vec{h})=\prod_ip(v_i|\vec{h})</script><p>Whatever the BM or the RBM, the state of a unit or you could say neuron could be 0 or 1. The probability to be 1 for each unit is:</p>
<script type="math/tex; mode=display">
p(h_j=1|\vec{v}) = \sigma(b_i + \sum_i v_iw_{ij})</script><p>Where $w_{ij}$ is the weight on the connection between unit $i$ nad $j$, and $v_i$ is the state of the unit, 1is on and 0 is off. $\sigma()$ function is:</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1+e^{-x}}</script><p>The detail information of how to get (4) please refer to <a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6530523.html">Here</a>.</p>
<h4 id="Training-Method-Gibbs-Sampling-amp-Contrastive-Divergence"><a href="#Training-Method-Gibbs-Sampling-amp-Contrastive-Divergence" class="headerlink" title="_Training Method: Gibbs Sampling &amp; Contrastive Divergence_"></a><strong>_Training Method: Gibbs Sampling &amp; Contrastive Divergence_</strong></h4><p>For the Constrastive Divergence algorithm please refer to <a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf">Notes On CD</a> and <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36607894/article/details/96635664">This</a>. </p>
<p>So for the gradient of $w_{ij}$, the equation is:</p>
<script type="math/tex; mode=display">
\Delta{w_{ij}} = \epsilon \times(<v_ih_j>_{data}-<v^{'}_ih^{'}_j>_{reconstruction})</script><h3 id="1-2-Maximum-Likelihood-Approximation"><a href="#1-2-Maximum-Likelihood-Approximation" class="headerlink" title="_1.2 Maximum Likelihood Approximation_"></a><strong>_1.2 Maximum Likelihood Approximation_</strong></h3><p><strong>MLA</strong> can approximate the model’s parameters when you have defined which model you chose. </p>
<p><img src="/image/article/MaxLike.jpeg" alt=""></p>
<center>
  Fig. 2 The visualization of maximum likelihood method.
</center>

<p>We can see <strong>RBM</strong> as a black box. Given an input vector and finally it will output a reconstructed vector. How do we guess the model parameters based on the information that we already know? Well, as we know, we have already understand the fundemental mechanism of the <strong>RBM</strong>. So we know its model, but we don’t know its parameters. Here, the maximum likelihood method can be used as shown in Fig. 2.</p>
<p>In the <strong>RBM</strong>, we want the output equals to the input. So, the input and label in this case are (<strong>X</strong>, <strong>X</strong>) because <strong>Y</strong>=<strong>X</strong>. Hence, we could derive the equation of $P(\vec{v_{out}})$:</p>
<script type="math/tex; mode=display">
P(\vec{v_{out}})= \sum_hP(\vec{v}, \vec{h})=\frac{\sum_he^{-E(\vec{v_{out},\vec{h}})}}{\sum_{\vec{v},\vec{h}}e^{-E(\vec{v}, \vec{h})}}</script><p>Write it in a log form:</p>
<script type="math/tex; mode=display">
L(W,a,b)=-\sum_iln(P(\vec{v^{(i)}_{out}}))</script><h2 id="2-Content"><a href="#2-Content" class="headerlink" title="_2. Content_"></a><strong>_2. Content_</strong></h2><p>The autoencoder presented by Hinton is used for dimensionality reduction. Compared with PCA, which finds the directions of greatest variance in the dataset and represents each data point by its coordinates along each of these directions. The visualization of an encoder is shown in Fig. 3.</p>
<p><img src="/image/article/encoder.png" alt=""></p>
<h3 id="2-1-The-current-issue-of-optimizing-the-autoencoder"><a href="#2-1-The-current-issue-of-optimizing-the-autoencoder" class="headerlink" title="_2.1 The current issue of optimizing the autoencoder_"></a><strong>_2.1 The current issue of optimizing the autoencoder_</strong></h3><p>When a autoencode has multiple layers, it is difficult to optimize the weight.</p>
<blockquote>
<p>If the initial weights are large, autoencoders will find poor local minima;</p>
<p>If the initial weights are small, the gradients will vanish.</p>
</blockquote>
<p>If the initial weights are close to a good solution, gradient descent works very well.</p>
<p><strong>QUESTION</strong>: How to find this initial weights?</p>
<p><strong>SOLUTION</strong>: Use pretraining procedure one layer at a time.</p>
<h3 id="2-2-The-pretrain-method"><a href="#2-2-The-pretrain-method" class="headerlink" title="_2.2 The pretrain method_"></a><strong>_2.2 The pretrain method_</strong></h3><p>Use <strong>RBM</strong> as an unit to train, and the contrastive divergence as learning method.</p>
<h3 id="2-3-Layer-by-layer-architecture"><a href="#2-3-Layer-by-layer-architecture" class="headerlink" title="_2.3 Layer by layer architecture_"></a><strong>_2.3 Layer by layer architecture_</strong></h3><p>After trained a <strong>RBM</strong>, we can use the hidden layer of the first <strong>RBM</strong> as the visible layer for the next <strong>RBM</strong>. You can connect it as many as possible. </p>
<h3 id="2-4-So-called-Deep-Belief-Network"><a href="#2-4-So-called-Deep-Belief-Network" class="headerlink" title="_2.4 So called Deep Belief Network_"></a><strong>_2.4 So called Deep Belief Network_</strong></h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37520918/article/details/102738465">Deep Belief Network</a></p>

    </div>

    
    
    

      <footer class="post-footer">
          <div class="post-tags">
              <a href="/tags/Paper/" rel="tag"># Paper</a>
              <a href="/tags/Deep-Belief-Network/" rel="tag"># Deep Belief Network</a>
              <a href="/tags/Boltzman-machine/" rel="tag"># Boltzman machine</a>
          </div>

        


        
    <div class="post-nav">
      <div class="post-nav-item">
    <a href="/2022/10/15/Intro/" rel="prev" title="Intro">
      <i class="fa fa-chevron-left"></i> Intro
    </a></div>
      <div class="post-nav-item"></div>
    </div>
      </footer>
    
  </article>
  
  
  



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          Table of Contents
        </li>
        <li class="sidebar-nav-overview">
          Overview
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E3%80%90Paper-Reading-Note%E3%80%91Reducing-the-dimensionality-of-data-with-neural-networks"><span class="nav-number">1.</span> <span class="nav-text">【Paper Reading Note】Reducing the dimensionality of data with neural networks</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#1-Background-Knowledge"><span class="nav-number">1.1.</span> <span class="nav-text">_1. Background Knowledge_</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#1-1-Boltzman-machine-amp-Restricted-Boltzman-machine"><span class="nav-number">1.1.1.</span> <span class="nav-text">_1.1 Boltzman machine &amp; Restricted Boltzman machine_</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#Mechanism-of-RBM"><span class="nav-number">1.1.1.1.</span> <span class="nav-text">_Mechanism of RBM_</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#Training-Method-Gibbs-Sampling-amp-Contrastive-Divergence"><span class="nav-number">1.1.1.2.</span> <span class="nav-text">_Training Method: Gibbs Sampling &amp; Contrastive Divergence_</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#1-2-Maximum-Likelihood-Approximation"><span class="nav-number">1.1.2.</span> <span class="nav-text">_1.2 Maximum Likelihood Approximation_</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#2-Content"><span class="nav-number">1.2.</span> <span class="nav-text">_2. Content_</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#2-1-The-current-issue-of-optimizing-the-autoencoder"><span class="nav-number">1.2.1.</span> <span class="nav-text">_2.1 The current issue of optimizing the autoencoder_</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-2-The-pretrain-method"><span class="nav-number">1.2.2.</span> <span class="nav-text">_2.2 The pretrain method_</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-3-Layer-by-layer-architecture"><span class="nav-number">1.2.3.</span> <span class="nav-text">_2.3 Layer by layer architecture_</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#2-4-So-called-Deep-Belief-Network"><span class="nav-number">1.2.4.</span> <span class="nav-text">_2.4 So called Deep Belief Network_</span></a></li></ol></li></ol></li></ol></div>
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">John Doe</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">9</span>
          <span class="site-state-item-name">posts</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">categories</span></a>
      </div>
      <div class="site-state-item site-state-tags">
        <span class="site-state-item-count">8</span>
        <span class="site-state-item-name">tags</span>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">John Doe</span>
</div>
  <div class="powered-by">Powered by <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://pisces.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a>
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/pisces.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
          load: ['[tex]/mhchem'],
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
          packages: {'[+]': ['mhchem']},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  

</body>
</html>
