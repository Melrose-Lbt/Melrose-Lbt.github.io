


<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>  【Paper Reading Note】Reducing the Dimensionality of Data With Neural Networks |    white.</title>
  <meta name="description" content="A minimalist theme for hexo.">
  <!-- 标签页图标 -->
  

  <!-- 图标库 -->
  <link href="https://cdn.jsdelivr.net/npm/remixicon@2.2.0/fonts/remixicon.css" rel="stylesheet">
  <!-- 动画库 -->
  
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/fushaolei/cdn-white@1.0/css/animate.css"/>
  
  <!-- css文件 -->
  
<link rel="stylesheet" href="/css/white.css">

  <!-- 代码高亮 -->
  
    
      
<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/highlight.js@10.1.1/styles/github.css">

    
  
<meta name="generator" content="Hexo 6.3.0"></head>


<body>

<div class="menu-outer">
    <div class="menu-inner">
      <div class="menu-site-name  animate__animated  animate__fadeInUp">
        <a href="/">
          white.
        </a>
        
      </div>
      <div class="menu-group">
        <ul class="menu-ul">
        
          <a href="/" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              HOME
            </li>
          </a>
        
          <a href="/archives" class="nav-link">
            <li class="menu-li  animate__animated  animate__fadeInUp">
              BLOG
            </li>
          </a>
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="sort">
             分类
             <div class="categories-outer " id="sort-div">
               <ul class="category-list"><li class="category-list-item"><a class="category-list-link" href="/categories/CUDA-Programming/">CUDA Programming</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Diary/">Diary</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Neutron/">Neutron</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Paper-reading-note/">Paper reading note</a></li><li class="category-list-item"><a class="category-list-link" href="/categories/Source-Code-Reading-Note/">Source Code Reading Note</a></li></ul>
             </div>
          </li>
        
        
          <li class="menu-li animate__animated  animate__fadeInUp" id="mobile-menu">
            <i class="ri-menu-line"></i>
          </li>
        
        </ul>

      </div>

    </div>
</div>
<div id="mobile-main" class="animate__animated  animate__fadeIn">
  <div class="mobile-menu-inner">
    <div class="mobile-menu-site-name animate__animated  animate__fadeInUp">
      <a href="/">
        white.
      </a>
    </div>
    <div class="mobile-menu-group" id="mobile-close">
      <i class="ri-close-line"></i>
    </div>

  </div>

  <div class="mobile-menu-div">
  
    <a href="/" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>HOME</span>
      </div>
    </a>
  
    <a href="/archives" class="mobile-nav-link">
      <div class="mobile-menu-child animate__animated  animate__fadeInUp">
        <span>BLOG</span>
      </div>
    </a>
  
  
  </div>


</div>

<div class="body-outer">
  <div class="body-inner">
    
<article class="post-inner">
  <div class="post-content-outer">
    <div class="post-intro">
      <div class="post-title animate__animated  animate__fadeInUp">【Paper Reading Note】Reducing the dimensionality of data with neural networks</div>
      <div class="meta-intro animate__animated  animate__fadeInUp">Oct 20 2022</div>
      
        <div class="post-cover"><span class="lazyload-img-span"><img data-src="/image/article/RBM.png" class="post-cover-img"></span></div>
      
    </div>
    <div class="post-content-inner">
      <div class="post-content-inner-space">

      </div>
      <div class="post-content-main animate__animated  animate__fadeInUp">
        <!-- top型目录 -->
        
        <h1 id="【Paper-Reading-Note】Reducing-the-dimensionality-of-data-with-neural-networks"><a href="#【Paper-Reading-Note】Reducing-the-dimensionality-of-data-with-neural-networks" class="headerlink" title="【Paper Reading Note】Reducing the dimensionality of data with neural networks"></a>【Paper Reading Note】Reducing the dimensionality of data with neural networks</h1><p>[TOC]</p>
<p><strong>Author</strong>: <strong>G.E. Hinton and R.R. Salakhutdinov</strong></p>
<blockquote>
<p><strong>Abstract</strong>: High-dimensional data can be converted to low-dimensional codes by training a multilayer neural network with a small central layer to reconstruct high-dimensional input vectors. Gradient descent can be used for fine-tuning the weights in such “auto encoder” networks, but this works well only if the initial weights are close to a good solution. We describe an effective way of initializing the weights that allows deep auto encoder networks to learn low-dimensional codes that work much better than principal components analysis as a tool to reduce the dimensionality of data.</p>
</blockquote>
<h2 id="1-Background-Knowledge"><a href="#1-Background-Knowledge" class="headerlink" title="1. Background Knowledge"></a><strong><em>1. Background Knowledge</em></strong></h2><h3 id="1-1-Boltzman-machine-amp-Restricted-Boltzman-machine"><a href="#1-1-Boltzman-machine-amp-Restricted-Boltzman-machine" class="headerlink" title="1.1 Boltzman machine &amp; Restricted Boltzman machine"></a><strong><em>1.1 Boltzman machine &amp; Restricted Boltzman machine</em></strong></h3><p>The first problem is what is the Boltzman machine. A <strong>Boltzmann machine</strong> is a network of symmetrically connected, neuron-like units that make stochastic decisions about whether to be on or off. A <strong>Restricted Boltzman machine</strong> is a network like the two layers of fully connected layer. The visualization of two machines are shown in <strong>Fig. 1</strong>.</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="/image/article/RBM.png" >
        </sapn>
      </p>
<center>Fig.1 Visualization of BM and RBM.</center>

<p>In this article, we mainly focus on the <strong>Restricted Boltzman machine</strong> (<strong>RBM</strong> in short).</p>
<h4 id="Mechanism-of-RBM"><a href="#Mechanism-of-RBM" class="headerlink" title="Mechanism of RBM"></a><strong><em>Mechanism of RBM</em></strong></h4><p>The <strong>Restricted Boltzman machine</strong> has two layer. The first layer is the visible layer, and the second layer is the hidden layer. Each layer has several nodes. Each node has two states, 1 or 0. We simply called these layer $v<em>i$, $h_j$, which means the ith node for the visible layer and the jth node for the hidden layer. The weight between the node $v_i$ and $h_j$ is $w</em>{ij}$. Each node also has bias.</p>
<p>The total energy for the given state vectors $\vec{v},\vec{h}$ is :</p>
<script type="math/tex; mode=display">
E(\vec{v}, \vec{h})= -\sum_ia_iv_i-\sum_jb_jh_j-\sum_{i,j}v_ih_jw_{ij}</script><p><strong>RBM</strong> is a probability-based model. In each time step, <strong>RBM</strong> has a certain state which can be descriped as $(\vec{v}, \vec{h})$. The probability distribution is :</p>
<script type="math/tex; mode=display">
p(\vec{v}, \vec{h})=\frac{e^{-E(\vec{v}, \vec{h} ) } }{\sum_{\vec{v}, \vec{h}}e^{-E(\vec{v}, \vec{h})}}</script><p>This is so called <strong>Boltzman distribution</strong>. It gives the probability when the particle in a certain state.</p>
<p>However, to calculate the (2) distribution is difficult. But to calculate the conditional distribution is relatively easier.</p>
<script type="math/tex; mode=display">
p(\vec{h}|\vec{v})=\prod_ip(h_i|\vec{v}) \\
p(\vec{v}|\vec{h})=\prod_ip(v_i|\vec{h})</script><p>Whatever the BM or the RBM, the state of a unit or you could say neuron could be 0 or 1. The probability to be 1 for each unit is:</p>
<script type="math/tex; mode=display">
p(h_j=1|\vec{v}) = \sigma(b_i + \sum_i v_iw_{ij})</script><p>Where $w_{ij}$ is the weight on the connection between unit $i$ nad $j$, and $v_i$ is the state of the unit, 1is on and 0 is off. $\sigma()$ function is:</p>
<script type="math/tex; mode=display">
\sigma(x) = \frac{1}{1+e^{-x}}</script><p>The detail information of how to get (4) please refer to <a target="_blank" rel="noopener" href="https://www.cnblogs.com/pinard/p/6530523.html">Here</a>.</p>
<h4 id="Training-Method-Gibbs-Sampling-amp-Contrastive-Divergence"><a href="#Training-Method-Gibbs-Sampling-amp-Contrastive-Divergence" class="headerlink" title="Training Method: Gibbs Sampling &amp; Contrastive Divergence"></a><strong><em>Training Method: Gibbs Sampling &amp; Contrastive Divergence</em></strong></h4><p>For the Constrastive Divergence algorithm please refer to <a target="_blank" rel="noopener" href="https://www.robots.ox.ac.uk/~ojw/files/NotesOnCD.pdf">Notes On CD</a> and <a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_36607894/article/details/96635664">This</a>. </p>
<p>So for the gradient of $w_{ij}$, the equation is:</p>
<script type="math/tex; mode=display">
\Delta{w_{ij}} = \epsilon \times(<v_ih_j>_{data}-<v^{'}_ih^{'}_j>_{reconstruction})</script><h3 id="1-2-Maximum-Likelihood-Approximation"><a href="#1-2-Maximum-Likelihood-Approximation" class="headerlink" title="1.2 Maximum Likelihood Approximation"></a><strong><em>1.2 Maximum Likelihood Approximation</em></strong></h3><p><strong>MLA</strong> can approximate the model’s parameters when you have defined which model you chose. </p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="/image/article/MaxLike.jpeg" >
        </sapn>
      </p>
<center>
  Fig. 2 The visualization of maximum likelihood method.
</center>

<p>We can see <strong>RBM</strong> as a black box. Given an input vector and finally it will output a reconstructed vector. How do we guess the model parameters based on the information that we already know? Well, as we know, we have already understand the fundemental mechanism of the <strong>RBM</strong>. So we know its model, but we don’t know its parameters. Here, the maximum likelihood method can be used as shown in Fig. 2.</p>
<p>In the <strong>RBM</strong>, we want the output equals to the input. So, the input and label in this case are (<strong>X</strong>, <strong>X</strong>) because <strong>Y</strong>=<strong>X</strong>. Hence, we could derive the equation of $P(\vec{v_{out}})$:</p>
<script type="math/tex; mode=display">
P(\vec{v_{out}})= \sum_hP(\vec{v}, \vec{h})=\frac{\sum_he^{-E(\vec{v_{out},\vec{h}})}}{\sum_{\vec{v},\vec{h}}e^{-E(\vec{v}, \vec{h})}}</script><p>Write it in a log form:</p>
<script type="math/tex; mode=display">
L(W,a,b)=-\sum_iln(P(\vec{v^{(i)}_{out}}))</script><h2 id="2-Content"><a href="#2-Content" class="headerlink" title="2. Content"></a><strong><em>2. Content</em></strong></h2><p>The autoencoder presented by Hinton is used for dimensionality reduction. Compared with PCA, which finds the directions of greatest variance in the dataset and represents each data point by its coordinates along each of these directions. The visualization of an encoder is shown in Fig. 3.</p>
<p>
        <span class="lazyload-img-span">
        <img   
           data-src="/image/article/encoder.png" >
        </sapn>
      </p>
<h3 id="2-1-The-current-issue-of-optimizing-the-autoencoder"><a href="#2-1-The-current-issue-of-optimizing-the-autoencoder" class="headerlink" title="2.1 The current issue of optimizing the autoencoder"></a><strong><em>2.1 The current issue of optimizing the autoencoder</em></strong></h3><p>When a autoencode has multiple layers, it is difficult to optimize the weight.</p>
<blockquote>
<p>If the initial weights are large, autoencoders will find poor local minima;</p>
<p>If the initial weights are small, the gradients will vanish.</p>
</blockquote>
<p>If the initial weights are close to a good solution, gradient descent works very well.</p>
<p><strong>QUESTION</strong>: How to find this initial weights?</p>
<p><strong>SOLUTION</strong>: Use pretraining procedure one layer at a time.</p>
<h3 id="2-2-The-pretrain-method"><a href="#2-2-The-pretrain-method" class="headerlink" title="2.2 The pretrain method"></a><strong><em>2.2 The pretrain method</em></strong></h3><p>Use <strong>RBM</strong> as an unit to train, and the contrastive divergence as learning method.</p>
<h3 id="2-3-Layer-by-layer-architecture"><a href="#2-3-Layer-by-layer-architecture" class="headerlink" title="2.3 Layer by layer architecture"></a><strong><em>2.3 Layer by layer architecture</em></strong></h3><p>After trained a <strong>RBM</strong>, we can use the hidden layer of the first <strong>RBM</strong> as the visible layer for the next <strong>RBM</strong>. You can connect it as many as possible. </p>
<h3 id="2-4-So-called-Deep-Belief-Network"><a href="#2-4-So-called-Deep-Belief-Network" class="headerlink" title="2.4 So called Deep Belief Network"></a><strong><em>2.4 So called Deep Belief Network</em></strong></h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/m0_37520918/article/details/102738465">Deep Belief Network</a></p>

        <!-- 分类文章 -->
        
          <div class="post-categoris-bottom">
            <div class="post-categoris-name">Paper reading note</div>
            <ul>
            
            
              
            
            
            
              
            
            
            
              
            
            
            
              
            
            
            
              
            
            
            
              
            
            
            
              
            
            
            
              
            
            
            
              
                <li class="me base">
                  <a  href="/2022/10/20/Reducing%20the%20dimensionality%20of%20data%20with%20neural%20networks/" class="post-categoris-bottom-link">
                  【Paper Reading Note】Reducing the dimensionality of data with neural networks
                </a>
                </li>
              
              
            
            
            </ul>

          </div>

        
      </div>
      <div class="post-content-inner-space">
        
      </div>
   </div>
    <!-- 评论 -->
    
  </div>
</article>
  </div>
</div>



<!-- 如果是home模式的话，不在首页就显示footer，如果不是home模式的话 所有都显示footer -->

  <div class="footer-outer animate__animated  animate__fadeInUp">
    <div class="footer-inner">
    <div class="footer-text">
    <p>Power by <a target="_blank" rel="noopener" href="http://hexo.io/">Hexo</a> Theme by <a target="_blank" rel="noopener" href="https://github.com/FuShaoLei/hexo-theme-white">White</a></p>

    </div>
    <div class="footer-contact">
    <ul class="footer-ul">
        
        <li class="footer-li">
            <a href="https://github.com/FuShaoLei/hexo-theme-white" target="_blank">
                <i class="ri-github-line"></i>
            </a>
        </li>
        
        <li class="footer-li">
            <a href="mailto:1563250958@qq.com" target="_blank">
                <i class="ri-mail-line"></i>
            </a>
        </li>
        
    </ul>
    </div>
    </div>
</div>






<script src="/js/white.js"></script>



    
      
<script src="https://cdn.jsdelivr.net/gh/highlightjs/cdn-release@9.18.1/build/highlight.min.js"></script>

      <script>hljs.initHighlightingOnLoad();</script>
    

<script type="text/x-mathjax-config">
    MathJax.Hub.Config({
        tex2jax: {
            inlineMath: [ ["$","$"], ["\\(","\\)"] ],
            skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code'],
            processEscapes: true
        }
    });
    MathJax.Hub.Queue(function() {
        var all = MathJax.Hub.getAllJax();
        for (var i = 0; i < all.length; ++i)
            all[i].SourceElement().parentNode.className += ' has-jax';
    });
</script>
<script src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
</body>
</html>
